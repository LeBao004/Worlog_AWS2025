[{"uri":"https://thienluhoan.github.io/workshop-template/4-eventparticipated/4.2-event2/","title":"Event 2","tags":[],"description":"","content":"Summary Report: ‚ÄúAI-powered planning, design, and coding for modern software development‚Äù Overview: An on-demand webinar from AWS Marketplace focused on bringing Generative AI into planning ‚Äì design ‚Äì coding across the SDLC to accelerate collaboration, automate testing, generate docs/diagrams, and enforce security via a zero-trust approach. (Source: AWS Marketplace webinar page ‚Äì On-demand).\nEvent Objectives Explain how GenAI reshapes planning, design, and coding on AWS. Show how to integrate GenAI into Agile/Scrum: sprint planning, backlog refinement, test generation. Demonstrate creating UI/UX mock-ups, architecture diagrams, and technical docs with AI for faster alignment. Share security/zero-trust practices for code analysis and architecture reviews. Speakers Harrison Kirby ‚Äî Ambassador, DevOps Institute Ronak Shah ‚Äî Principal Solutions Architect, AWS Highlights 1) GenAI across the SDLC Planning \u0026amp; Design: AI proposes architecture options and generates diagrams/docs; supports early decision-making. Coding \u0026amp; Testing: Real-time code suggestions, unit/integration test generation, fewer fix-rework cycles. Collaboration: Rapid UI/UX mock-ups to align architects, developers, and designers. 2) ‚ÄúAttendees will learn‚Äù (from the webinar page) Embedding GenAI into Agile workflows for sprint planning, backlog refinement, test generation. Using AI-generated visuals/diagrams/code to accelerate cross-functional collaboration. Applying security frameworks and zero-trust principles with AI for architecture reviews and code analysis. 3) Practical tie-ins (from your supporting document) Governance Copilot: flags scope creep/budget drift; auto-creates minutes and risk registers. Smarter Estimation: learns from historical projects; outputs best/worst-case ranges, not a single point. Scope Clarifier: NLP capture/analysis to detect ambiguous/conflicting/missing requirements before scope lock. Dependency Radar: AI-graph mapping of team/vendor/module dependencies to avoid bottlenecks. Auto-documentation: keeps UML/sequence/workflow/API docs in sync with code/design changes. Key Takeaways Vision ‚Üí Value: anchor every GenAI effort to clear KPIs/ROI (speed, cost, quality). Data-first: retrieval/embedding/rerank quality drives output quality. Security-by-design: zero-trust, access control, PII protection, content moderation, cost/token visibility. Observability \u0026amp; Eval: tracing, online/offline evaluation, continuous feedback loops. Applying to Work Pilot 1‚Äì2 GenAI use cases over 6‚Äì8 weeks with go/no-go gates (quality, latency, cost/interaction, adoption). Enable a governance copilot (scope/budget alerts) and auto-documentation from the first sprint. Standardize estimations with historical data; publish 2‚Äì3 scenarios instead of one number. Use a scope clarifier for all requirement sessions; run a dependency radar before major design milestones. Event Experience The webinar shows how to operationalize AI‚Äîfrom documentation/architecture to code/test and security‚Äîhelping teams reduce process friction and shorten lead time while maintaining safety and scalability on AWS.\nSome event photos Add your screenshots/photos here\nIn short, the session outlines measurable GenAI steps across the SDLC: AI does the heavy lifting, while humans supervise and decide.\n"},{"uri":"https://thienluhoan.github.io/workshop-template/","title":"Internship Report","tags":[],"description":"","content":"Internship Report Student Information: Full Name: LE HO GIA BAO\nPhone Number: 0398348387\nEmail: baolhgse184518@fpt.edu.vn\nUniversity: FPT University\nMajor: Information Technology\nClass: AWS082025\nInternship Company: Amazon Web Services Vietnam Co., Ltd.\nInternship Position: FCJ Cloud Intern\nInternship Duration: From 12/08/2025 to 12/11/2025\nReport Content Worklog Proposal Translated Blogs Events Participated Workshop Self-evaluation Sharing and Feedback "},{"uri":"https://thienluhoan.github.io/workshop-template/4-eventparticipated/4.1-event1/","title":"Event 1","tags":[],"description":"","content":"Summary Report: ‚ÄúAWS Cloud Day Vietnam 2025 ‚Äì AI Edition‚Äù Theme \u0026amp; Tagline: New Age Vietnam: From Vision to Value ‚Äî turning AI vision into measurable value.\nFocus: Generative AI with complementary Cloud capabilities (data, security, infrastructure, modern apps).\nEvent Objectives Drive Vision ‚Üí Value: tie GenAI to clear KPIs/ROI (speed, cost, quality). Present GenAI reference architectures (RAG, agentic workflows, data pipelines, guardrails). Define a practical adoption path: Idea ‚Üí PoC ‚Üí Pilot ‚Üí Scale with governance \u0026amp; data safety. Equip builders/engineers and IT leaders with AWS best practices (observability, cost control). Speakers AWS Vietnam leaders \u0026amp; specialists (AI/GenAI, Data, Security, AppMod). Customer speakers from Vietnam (transformation \u0026amp; GenAI use cases). Partners (deployment playbooks, cost optimization, operations). Key Highlights Central theme: Generative AI on AWS Amazon Bedrock / Amazon Q \u0026amp; Guardrails: rapid start, security, compliance. Data-first: retrieval/rerank/embeddings quality drives output quality. Cloud consistency: build on AWS foundations (identity, networking, storage, monitoring) for security \u0026amp; scale. Representative tracks GenAI Foundations \u0026amp; Architecture: RAG, tool-use/agents, evaluation \u0026amp; observability. Data, Security \u0026amp; Governance: PII protection, access control, moderation, cost/token tracking. App Modernization for AI: service decomposition, API-first, event-driven; making apps AI-ready. Builders Hands-on: demos/mini-labs for internal assistants and semantic search. Key messages Vision ‚Üí Value: avoid demo-only efforts; anchor to KPIs from day one. Data before models: optimize pipelines, indexing, caching, and latency. Safety \u0026amp; compliance: guardrails, auditability, continuous monitoring \u0026amp; evaluation. Key Takeaways Design Mindset Business-first: small, high-impact problems with clear owners \u0026amp; data. Ubiquitous language: align business‚Äìtech and lock KPIs/ROI early. Technical Architecture Prefer RAG and agentic patterns; separate orchestrator from tools. Observability \u0026amp; Eval: tracing, offline/online evaluation, feedback loops. Cost \u0026amp; Performance: fit-for-purpose models, batching/caching, prompt/context optimization, latency control. Modernization Strategy Roadmap PoC ‚Üí pilot ‚Üí limited rollout ‚Üí scale with exit criteria per stage. Build on AWS foundations for security, resilience, and scalability. Applying to Work Shortlist 1‚Äì2 GenAI use cases tied to clear KPIs; prepare data \u0026amp; access. Run a 6‚Äì8 week PoC with go/no-go gates (quality, latency, cost/interaction, adoption). Ops plan: monitoring, guardrails, human-in-the-loop, continuous improvement. Event Experience The event offered a clear path to move from vision to value: deploying GenAI into real workflows with security, cost awareness, and business alignment. Talks and demos showed how to standardize the data pipeline, implement RAG/agents, and measure impact to unlock pilot \u0026amp; scale.\nSome event photos Add your event photos here\nIn short, the ‚ÄúAI Edition‚Äù underscores that Generative AI creates real value when paired with the right data, architecture, governance, and KPIs on AWS.\n"},{"uri":"https://thienluhoan.github.io/workshop-template/5-workshop/5.3-setup-amazon-bedrock/5.3.1-check-model/","title":"Enable and Test Claude Haiku 4.5","tags":[],"description":"","content":"1. Navigate to Amazon Bedrock Console\nOpen the Amazon Bedrock Console\n2. Click on Model Catalog and Search for Claude Haiku 4.5\nIn the Model Catalog, search for \u0026ldquo;Claude\u0026rdquo; to find the Claude Haiku 4.5 model.\n3. Click on Claude Haiku 4.5 and Find the Model ID\nNote the Model ID - you will need this for your Lambda function later.\n4. Test the Model in Playground (Optional)\nYou can click \u0026ldquo;Open in playground\u0026rdquo; to test the model before integrating it into your application.\nImportant: You no longer need to request model access separately. With an IAM account that has AWS Marketplace permissions, you can directly use the API of any model in Amazon Bedrock. The model will be activated automatically upon first invocation.\nReference Documentation:\nClaude Model Parameters Bedrock API Reference For Policy, leave the default option, Full Access, to allow full access to the service. You will deploy a VPC endpoint policy in a later lab module to demonstrate restricting access to S3 buckets based on policies. Do not add a tag to the VPC endpoint at this time. Click Create endpoint, then click x after receiving a successful creation message. "},{"uri":"https://thienluhoan.github.io/workshop-template/5-workshop/5.1-workshop-overview/","title":"Workshop Overview","tags":[],"description":"","content":"Workshop Overview In this workshop, you will build a complete AI Chatbot using serverless architecture on AWS. The chatbot uses Claude Haiku 4.5 and is deployed completely serverless with low cost and automatic scaling capabilities.\nWorkshop Modules Module 1: Setup Amazon Bedrock\nEnable Claude Haiku 4.5 model access Understand inference profiles Test model via AWS Console Module 2: Create Lambda Function\nCreate Node.js 24 Lambda function Deploy chatbot backend code Configure IAM role with Bedrock permissions Set environment variables Module 3: Configure API Gateway\nCreate REST API Configure POST /chat endpoint Enable CORS Test API with Postman Module 4: Deploy Frontend to S3\nCreate S3 bucket Enable static website hosting Upload HTML chatbot UI Configure public access Module 5: Testing \u0026amp; Debugging\nTest end-to-end flow View CloudWatch logs Architecture Overview ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\r‚îÇ Browser ‚îÇ\r‚îÇ (HTML/JS) ‚îÇ\r‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\r‚îÇ HTTPS\r‚ñº\r‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\r‚îÇ Amazon S3 ‚îÇ ‚Üê Static Website Hosting\r‚îÇ (Frontend HTML) ‚îÇ\r‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\r‚îÇ API Call (POST /chat)\r‚ñº\r‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\r‚îÇ API Gateway ‚îÇ ‚Üê REST API Endpoint\r‚îÇ (HTTPS Endpoint) ‚îÇ\r‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\r‚îÇ Invoke\r‚ñº\r‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\r‚îÇ AWS Lambda ‚îÇ ‚Üê Backend Logic (Node.js 24)\r‚îÇ (Chatbot Handler) ‚îÇ\r‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\r‚îÇ InvokeModel API\r‚ñº\r‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\r‚îÇ Amazon Bedrock ‚îÇ ‚Üê AI Engine\r‚îÇ (Claude Haiku 4.5) ‚îÇ\r‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\r‚îÇ\r‚ñº\r‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\r‚îÇ CloudWatch ‚îÇ ‚Üê Monitoring \u0026amp; Logs\r‚îÇ (Logs \u0026amp; Metrics) ‚îÇ\r‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò Key Components Amazon Bedrock (AI Engine) - Provides Claude Haiku 4.5 model through simple API. You call InvokeModel API to send messages and receive intelligent AI responses.\nAWS Lambda (Backend Logic) - Runs Node.js 24 code to process requests from API Gateway. Lambda validates input (message length, history limits), formats prompts for Bedrock, calls Bedrock API, and handles errors.\nAPI Gateway (REST API Endpoint) - Creates public HTTPS endpoint (POST /chat) for frontend calls. API Gateway handles CORS configuration, routes requests to Lambda, and provides throttling to protect backend from abuse.\nAmazon S3 (Frontend Hosting) - Hosts static website (HTML) for chatbot UI. Users access chatbot via browser, interface sends messages to API Gateway, and displays responses from AI.\nCloudWatch (Monitoring \u0026amp; Debugging) - Automatically collects logs from Lambda execution, allowing you to debug errors.\nWorkflow User enters message into chatbot UI (hosted on S3) JavaScript sends POST request to API Gateway endpoint API Gateway invokes Lambda function Lambda validates input, formats prompt, calls Bedrock InvokeModel API Bedrock processes with Claude Haiku 4.5, returns AI response Lambda returns response to API Gateway ‚Üí Browser CloudWatch logs entire process "},{"uri":"https://thienluhoan.github.io/workshop-template/3-blogstranslated/3.1-blog1/","title":"Blog 1","tags":[],"description":"","content":"k6 Document: Cloud Cost Optimization Strategies and Tools for AWS, Azure, and GCP One of the biggest challenges in the cloud is cost optimization. If you don‚Äôt control costs, they can skyrocket. (And no one wants to be summoned by the finance team or the CFO to explain runaway cloud spending.)\nThat‚Äôs why it‚Äôs best to proactively manage cloud costs and optimize from day one. In this article, we‚Äôll look at practical ways to manage costs across the three major providers: Azure, AWS, and GCP.\nWhat happens when you don‚Äôt control cloud costs Failing to optimize early can lead to very real, painful problems:\nRunaway spending. Without limits or budgets, costs can climb rapidly. Budget overruns and missed KPIs. Projects overspend, casting doubt on ‚Äúcloud-first‚Äù strategies. Higher risk of service suspension. Especially for startups or capped usage, late payments or overages can trigger throttling or pauses. Overprovisioned and wasted resources. You pay for unused compute, storage, and networking that deliver no business value. Reduced flexibility. Overspending forces overly strict controls just to ‚Äúcourse-correct.‚Äù Slower time-to-market. Teams hesitate to launch resources because costs are unpredictable. Lost competitive edge. Poor cloud efficiency can make you pay 2‚Äì3√ó competitors for the same capability. Eroded leadership trust. Persistent overruns without comparable value make leaders question cloud as a strategy. Optimize before things get worse The most dangerous aspect of unoptimized cloud costs is compounding over time‚Äîlike interest. Small gaps become heavy burdens, shrinking your ability to innovate. The longer you wait, the more expensive and complex the cleanup becomes‚Äîclassic technical debt.\nKey mindset: treat cost optimization like tending a garden‚Äîregular care, pruning, and adjustments. Start with the biggest, easiest wins, then move into deeper optimizations.\nPlatform-agnostic cost strategies Each major cloud offers tools and methods to help you improve cost and performance. Below are general strategies, followed by provider-specific tips for AWS, Azure, and GCP.\n1) Right-size resources Inventory what you run and how it‚Äôs used. If something is underutilized, downsize or reconfigure it.\nAzure: use Azure Advisor to find underutilized VMs. AWS: use Compute Optimizer or Trusted Advisor for EC2, RDS, and Lambda right-sizing. GCP: use Recommender to tune machine types or autoscaler settings for Compute Engine. üëâ Kick off your AWS journey with Managing Compute Costs in AWS (Pluralsight).\n2) Clean up idle resources Cloud can turn into Hoarders fast. Teams create resources; people change roles; VMs, databases, and storage linger unowned. Nobody deletes them for fear of breaking something‚Äîclassic cloud sprawl.\nFix it:\nKeep good documentation. Audit regularly and remove what‚Äôs not needed. Automate cleanup with scripts or policy as code (e.g., Terraform + scheduled jobs). 3) Optimize storage costs Storage often drives surprise bills. Teams pick the wrong tiers or leave orphaned data in expensive classes.\nDo this:\nTrain teams on tiers and their purposes. Define clear allocation and ownership policies. Use archive tiers for cold data (backups, logs, compliance). Automation helpers:\nAzure: Blob Lifecycle Management ‚Üí shift to Cool/Archive. AWS: S3 Lifecycle Policies, Intelligent-Tiering ‚Üí move to Glacier/Deep Archive. GCP: Object Lifecycle Management ‚Üí Standard ‚Üí Nearline/Coldline. 4) Reduce network egress and fix traffic patterns Network fees are like toll roads‚Äîsmall individually, big in aggregate. Egress charges can silently drain budgets. Understand flows and architect to minimize unnecessary data movement.\nTactics:\nData locality planning: keep data near compute. E.g., app in us-east with DB in Europe = expensive egress. Compression \u0026amp; optimization: enable gzip, optimize images/video, dedupe backups. Traffic analysis: use AWS Cost Explorer, Azure Cost Management, GCP Cloud Billing to spot anomalies. Caching: CDN, app-level caches, DB query caching, API caching. Provider aids:\nAzure: ExpressRoute, Traffic Manager. AWS: VPC Endpoints, Global Accelerator, consolidate data regions. GCP: Private Google Access, co-locate services to cut egress. 5) Use AI-powered recommendations Each cloud ships ‚Äúcost whisperers‚Äù that suggest optimizations:\nAzure: Azure Advisor AWS: Compute Optimizer, Cost Anomaly Detection GCP: Recommender API, Active Assist These aren‚Äôt ‚Äúnice-to-haves‚Äù‚Äîthey‚Äôre your first line of defense against cost creep. Review and apply recommendations regularly.\n6) Continuous cost monitoring Cost tracking isn‚Äôt a one-time task‚Äîit‚Äôs continuous. Ongoing monitoring catches inefficiencies early and prevents surprise bills. Set up budgets, alerts, and forecasting, using native or third-party tools.\nNative tools:\nAzure: Cost Management, Azure Monitor AWS: CloudWatch, Cost Explorer, Budgets GCP: Billing Dashboard, Cloud Monitoring, BigQuery exports Provider tips:\nAzure: Use Azure Hybrid Benefit (for Windows Server licenses) + Cost Management analytics. AWS: Lean on Trusted Advisor + Cost Explorer for patterns and anomalies. GCP: Use sustained use discounts for long-running workloads and per-second billing for short jobs. Conclusion Hopefully this gives you practical strategies to start optimizing cloud costs today. You now have a strategy toolbox for AWS, Azure, and GCP. Pick your biggest cost driver and start there‚Äîyour budget will thank you.\nLevel up your team‚Äôs cloud skills with Pluralsight‚Äîhands-on labs, deep resources, and comprehensive cloud courses.\nAuthor\nSteve Buchanan ‚Äî Principal Program Manager at a global technology enterprise focused on improving cloud computing. Pluralsight author; author of eight technical books; Onalytica Who‚Äôs Who in Cloud? Top 50; former 10-time Microsoft MVP. Speaker at DevOps Days, Open Source North, Midwest Management Summit (MMS), Microsoft Ignite, BITCon, Experts Live Europe, OSCON, Inside Azure Management, keynote at Minnebar 18, and many user groups. Featured on numerous podcasts and in publications including the Star Tribune (5th-largest U.S. newspaper). He blogs at www.buchatech.com.\n"},{"uri":"https://thienluhoan.github.io/workshop-template/3-blogstranslated/3.2-blog2/","title":"Blog 2","tags":[],"description":"","content":"Non-disruptive streaming with Olyzon and AWS Elemental MediaTailor preconditioned ads Author: Thomas Buatois ‚Äî June 24, 2025\nCategories: AWS Elemental MediaTailor, AWS Marketplace, Industries, Media \u0026amp; Entertainment, Media Services, Monetization, Partner solutions\nPermalink | Share\nStreaming video providers face the ongoing challenge of maximizing ad revenue while maintaining viewer engagement. The challenge lies in delivering seamless advertisements (ads) that integrate into content without causing buffering, quality degradation, or playback interruptions that drive viewers away.\nAWS Partner ‚Äì Olyzon has developed an innovative solution using preconditioned ads from AWS Elemental MediaTailor to achieve this balance.\nIntroducing preconditioned ads for enhanced ad insertion MediaTailor supports preconditioned ads, a feature that enables custom control over the ad transcoding process. This feature, also known as bring-your-own-ads through VAST responses, allows ad decision servers (ADS) to include HLS and DASH manifest URLs for multi-bitrate ad streams that have been pre-transcoded directly in the VAST XML creative file attribute.\nMediaTailor can now stitch these pre-transcoded ad creatives into the manifest without further transcoding. Previously, MediaTailor could only dynamically transcode ads to match the content stream at the time of insertion, causing potential delays in the ad delivery process.\nHow preconditioned ads transform the workflow In the traditional ad insertion workflow, MediaTailor must dynamically transcode ads to match the content stream, store them, and then stitch them into the live stream. This process introduces delays because MediaTailor must wait to receive ads from the ADS\u0026rsquo;s VAST responses before starting transcoding and stitching.\nPreconditioned ads help reduce the time required to insert ads into content by eliminating the transcoding step. These ads must be prepared to match the content stream and already transcoded before use with MediaTailor. The ADS VAST response must include direct links to the pre-transcoded HLS and DASH manifests. MediaTailor only needs to register the ad and stitch it into the content stream, reducing the time between receiving the VAST response and completing ad insertion.\nVAST response requirements for preconditioned ads Unlike typical VAST responses, where MediaTailor receives single creatives and transcodes them into multiple bitrates itself, the ADS must provide correctly formatted VAST responses with pre-transcoded assets.\nVAST Example:\n\u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;VAST xmlns:xsi=\u0026#34;[http://www.w3.org/2001/XMLSchema-instance](http://www.w3.org/2001/XMLSchema-instance)\u0026#34; version=\u0026#34;3.0\u0026#34;\u0026gt; \u0026lt;Ad id=\u0026#34;ad1\u0026#34;\u0026gt; \u0026lt;InLine\u0026gt; \u0026lt;AdSystem\u0026gt;ExampleAdSystem\u0026lt;/AdSystem\u0026gt; \u0026lt;AdTitle\u0026gt;ad1\u0026lt;/AdTitle\u0026gt; \u0026lt;Impression\u0026gt;\u0026lt;![CDATA[[https://example-impression.amazonaws.com](https://example-impression.amazonaws.com)]]\u0026gt;\u0026lt;/Impression\u0026gt; \u0026lt;AdServingId\u0026gt;de8e0d33-9c72-4d77-bb3a-f7e566ffc605\u0026lt;/AdServingId\u0026gt; \u0026lt;Creatives\u0026gt; \u0026lt;Creative id=\u0026#34;creativeId1\u0026#34; sequence=\u0026#34;1\u0026#34;\u0026gt; \u0026lt;Linear skipoffset=\u0026#34;00:00:05\u0026#34;\u0026gt; \u0026lt;Duration\u0026gt;00:00:30\u0026lt;/Duration\u0026gt; \u0026lt;MediaFiles\u0026gt; \u0026lt;MediaFile delivery=\u0026#34;progressive\u0026#34; width=\u0026#34;1280\u0026#34; height=\u0026#34;720\u0026#34; type=\u0026#34;video/mp4\u0026#34; bitrate=\u0026#34;533\u0026#34; scalable=\u0026#34;true\u0026#34; maintainAspectRatio=\u0026#34;true\u0026#34;\u0026gt;\u0026lt;![CDATA[[https://example-ad-origin.amazonaws.com/ad1/ad1.mp4](https://example-ad-origin.amazonaws.com/ad1/ad1.mp4)]]\u0026gt;\u0026lt;/MediaFile\u0026gt; \u0026lt;MediaFile delivery=\u0026#34;streaming\u0026#34; width=\u0026#34;1280\u0026#34; height=\u0026#34;720\u0026#34; type=\u0026#34;application/dash+xml\u0026#34; bitrate=\u0026#34;533\u0026#34; scalable=\u0026#34;true\u0026#34; maintainAspectRatio=\u0026#34;true\u0026#34;\u0026gt;\u0026lt;![CDATA[[https://example-ad-origin.amazonaws.com/ad1/index.mpd](https://example-ad-origin.amazonaws.com/ad1/index.mpd)]]\u0026gt;\u0026lt;/MediaFile\u0026gt; \u0026lt;MediaFile delivery=\u0026#34;streaming\u0026#34; width=\u0026#34;640\u0026#34; height=\u0026#34;360\u0026#34; type=\u0026#34;application/x-mpegURL\u0026#34; bitrate=\u0026#34;262\u0026#34; scalable=\u0026#34;true\u0026#34; maintainAspectRatio=\u0026#34;true\u0026#34;\u0026gt;\u0026lt;![CDATA[[https://example-ad-origin.amazonaws.com/ad1/index_low.m3u8](https://example-ad-origin.amazonaws.com/ad1/index_low.m3u8)]]\u0026gt;\u0026lt;/MediaFile\u0026gt; \u0026lt;MediaFile delivery=\u0026#34;streaming\u0026#34; width=\u0026#34;2560\u0026#34; height=\u0026#34;1440\u0026#34; type=\u0026#34;application/x-mpegURL\u0026#34; bitrate=\u0026#34;1066\u0026#34; scalable=\u0026#34;true\u0026#34; maintainAspectRatio=\u0026#34;true\u0026#34;\u0026gt;\u0026lt;![CDATA[[https://example-ad-origin.amazonaws.com/ad1/index_high.m3u8](https://example-ad-origin.amazonaws.com/ad1/index_high.m3u8)]]\u0026gt;\u0026lt;/MediaFile\u0026gt; \u0026lt;/MediaFiles\u0026gt; \u0026lt;/Linear\u0026gt; \u0026lt;/Creative\u0026gt; \u0026lt;/Creatives\u0026gt; \u0026lt;/InLine\u0026gt; \u0026lt;/Ad\u0026gt; \u0026lt;/VAST\u0026gt; "},{"uri":"https://thienluhoan.github.io/workshop-template/3-blogstranslated/3.3-blog3/","title":"Blog 3","tags":[],"description":"","content":" ‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy it verbatim for your report, including this warning.\nRegister Now: Updated Exam and New Name for Cloud Operations Certification Author: Lauren Ebbin ‚Äî July 15, 2025 Categories: Announcements, AWS Training and Certification, DevOps Permalink | Share\nEdited on September 9, 2025, to announce that registration is now open and new exam preparation resources are available on AWS Skill Builder.\nRegistration is now open for the AWS Certified CloudOps Engineer ‚Äì Associate exam (previously known as the AWS Certified SysOps Administrator ‚Äì Associate) to align with the latest skills and knowledge in monitoring and maintaining Amazon Web Services (AWS) workloads. The last day to take the AWS Certified SysOps Administrator ‚Äì Associate (SOA-C02) exam will be September 29, 2025.\nFrom SysOps to CloudOps We are renaming this certification to AWS Certified CloudOps Engineer ‚Äì Associate to reflect the evolving nature of cloud operations and the change in industry terminology. This change aims to enhance the relevance and credibility of individuals who earn this certification, highlighting their knowledge and skills in deploying, operating, and maintaining workloads on AWS.\nFrom SOA-C02 to SOA-C03 We have updated the exam with the help of subject matter experts (SMEs) and will release a new exam guide on September 9, 2025. A key difference between AWS Certified SysOps Administrator ‚Äì Associate (SOA-C02) and AWS Certified CloudOps Engineer ‚Äì Associate (SOA-C03) is that containers are now in scope for SOA-C03.\nWe have included more modern services and features, increased emphasis on multi-account, multi-Region architectures, and more on automation and infrastructure as code. No task statements were removed between exam versions, but the new content outline is more detailed with some reorganization of the task statements.\nCertification maintenance and badges The name change to AWS Certified CloudOps Engineer ‚Äì Associate will only apply to those who pass the SOA-C03 exam. The name will not be retroactively changed for current holders of the AWS Certified SysOps Administrator ‚Äì Associate certification. Here is what you need to know about certification validity, recertification, and maintaining certified status:\nIndividuals who earned the AWS Certified SysOps Administrator ‚Äì Associate certification are not required to earn the AWS Certified CloudOps Engineer ‚Äì Associate certification. They can continue to display their certification badge, which will remain active until its expiration date. Individuals who earned the AWS Certified SysOps Administrator ‚Äì Associate certification and subsequently earn the AWS Certified CloudOps Engineer ‚Äì Associate certification will have a separate badge for each. Their AWS Certified SysOps Administrator ‚Äì Associate certification will remain active until its expiration date. In the future, these individuals can choose to: Continue to recertify the AWS Certified CloudOps Engineer ‚Äì Associate certification by passing the latest version of the exam. Recertify both certifications by upgrading to the AWS Certified DevOps Engineer ‚Äì Professional certification before the certifications‚Äô expiration date. Individuals with active AWS Certified DevOps Engineer ‚Äì Professional and AWS Certified SysOps Administrator ‚Äì Associate certifications can continue to recertify both by passing the latest version of the AWS Certified DevOps Engineer ‚Äì Professional exam. Get ready with AWS Skill Builder The Exam Prep Plan: AWS Certified CloudOps Engineer ‚Äì Associate (SOA-C03) is now available on AWS Skill Builder.\nThe Exam Prep Plan includes:\npractice assessments with exam-style questions hands-on practice through AWS SimuLearn review courses of each exam domain The Plan also references role-based training to refresh your AWS knowledge and skills.\nResources AWS Training and Certification Get Trained Get Certified Develop Your Team AWS Partner Training AWS Educate "},{"uri":"https://thienluhoan.github.io/workshop-template/3-blogstranslated/3.4-blog4/","title":"Blog 4","tags":[],"description":"","content":" ‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nGetting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, ‚ÄúGetting Started with Healthcare Data Lakes: Diving into Amazon Cognito‚Äù, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the ‚Äúpub/sub hub.‚Äù\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function ‚Üí ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda ‚Äútrigger‚Äù subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 ‚Üí JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "},{"uri":"https://thienluhoan.github.io/workshop-template/3-blogstranslated/3.5-blog5/","title":"Blog 5","tags":[],"description":"","content":" ‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nGetting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, ‚ÄúGetting Started with Healthcare Data Lakes: Diving into Amazon Cognito‚Äù, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the ‚Äúpub/sub hub.‚Äù\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function ‚Üí ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda ‚Äútrigger‚Äù subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 ‚Üí JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "},{"uri":"https://thienluhoan.github.io/workshop-template/3-blogstranslated/3.6-blog6/","title":"Blog 6","tags":[],"description":"","content":" ‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nGetting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, ‚ÄúGetting Started with Healthcare Data Lakes: Diving into Amazon Cognito‚Äù, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the ‚Äúpub/sub hub.‚Äù\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function ‚Üí ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda ‚Äútrigger‚Äù subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 ‚Üí JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "},{"uri":"https://thienluhoan.github.io/workshop-template/1-worklog/1.1-week1/","title":"Week 1 Worklog","tags":[],"description":"","content":"Week 1 Objectives: Make new friends and find your group. Learn how to self-study from scratch and complete the challenge to get $100 Free tier. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Find and get to know your group mates. - Read and learn the rules and regulations on how to fill out the registration form at the office. - Read and learn the rules and regulations on how to fill out the registration form at the office. - Learn how to get to the company and get a visitor card, and park your vehicle at the internship company. 08/09/2025 08/09/2025 https://cloudjourney.awsstudygroup.com/ 3 - Read and learn more about AWS to know what the upcoming program will do like workshops or projects\u0026hellip;.. - Find the link to write a worklog and write a worklog. 09/09/2025 09/09/2025 https://workshop-sample.fcjuni.com/1-worklog/ 4 - Learn and practice how to create an AWS Free Tier account. - Learn about root user and IMA account. - Practice: + Create AWS account. + Learn how to create and delete VCP. + Learn how to create and delete EC2. 10/09/2025 10/09/2025 https://aws.amazon.com/profile 5 - Basic understanding and challenges that the account offers to receive an additional $100. - Read and learn the first 2 challenges to receive $40. - Practice: + Take the challenge and complete challenge 2. + Take the challenge and complete challenge 3. + Learn how to draw clouds on draw.io. 11/09/2025 11/09/2025 https://us-east-1.console.aws.amazon.com/billing/home#/credits 6 - Learn and how to do 3 challenges to get the remaining $60 on your AWS account. - Practice: + Complete Challenge 3. + Complete Challenge 4. + Complete Challenge 5. 12/09/2025 12/09/2025 https://us-east-1.console.aws.amazon.com/billing/home#/credits Week 1 Achievements: Register to go to the office with the team to learn and know how to use aws.\nSuccessfully created and configured an AWS Free Tier account.\nGet familiar with aws root and aws IMA accounts, learn how to search, access and use services, complete tasks to receive rewards from that service.\nInstalled and configured AWS and know how to use the services on the computer, including:\nEC2. VCP. Learn how to create and delete services to avoid being charged. Learn and draw from the sample from draw.io about cloud drawings. \u0026hellip; Used AWS CLI to perform basic operations such as:\nKnow how to manage accounts. Check daily balance to see if there are any services running in the background that cost money. View EC2 service. Learn how to create and delete services to avoid being charged. Check information about running services. \u0026hellip; Acquired the ability to connect between the web interface and CLI to manage AWS resources in parallel.\n"},{"uri":"https://thienluhoan.github.io/workshop-template/1-worklog/1.2-week2/","title":"Week 2 Worklog","tags":[],"description":"","content":"Week 2 Objectives: Basic understanding of AWS Cloud9, S3, RDS (Read and learn). Complete Lab S3 (Tuesday). Participate in Cloud Day Vietnam event and learn about AI trends (Wednesday). Complete Lab RDS (Fixed and completed on Friday). Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Read and explore AWS Cloud9 (note: access is currently closed in this application). - Read and learn Amazon S3 to practice and do labs. - Continue reading and learning Amazon RDS to practice and do labs. 14/09/2025 16/09/2025 https://000057.awsstudygroup.com/vi/1-introduce/ 3 - Complete the lab and understand the purpose of the S3 Lab: - Practice: + Store objects. + Use data management features. + Apply management and security. + \u0026hellip; 17/09/2025 17/09/2025 https://us-east-1.console.aws.amazon.com/s3/bucket/create?region=us-east-1\u0026bucketType=general 4 - Join the Cloud Day Vietnam event and learn a lot from the event (also the first event at FCJ). - Learn current trends and how Vietnamese companies and business leaders are using AI to automate, increase revenue, and optimize processes. 18/09/2025 18/09/2025 https://vmxwvcrs.r.us-east-1.awstrack.me/L0/https:%2F%2Femail.awscloud.com%2FMTEyLVRaTS03NjYAAAGc9dyF5F1pH9rPDZl68ocGtzzZO0RNKl9nlzWVoHl19AV5GA2hLENGpcAopvy9k4xd-U4R2vg=/1/0100019956e3db6c-8f5a7a13-b84e-4b95-b82f-414391a6af17-000000/aOaA73q6gzphysKAm7ScQrPg4Fo=444 5 - Worked on the RDS lab but faced errors all day and couldn‚Äôt fix them. - Read the web instructions and prepared to practice to complete the lab. - Practice: + Follow the instructions. + Create tables as instructed. + \u0026hellip;; 19/09/2025 19/09/2025 https://us-east-1.console.aws.amazon.com/rds/home?region=us-east-1# 6 - Practice: + Fixed the errors + Recreated tables and continued completing RDS + Completed the RDS lab 20/09/2025 20/09/2025 https://us-east-1.console.aws.amazon.com/rds/home?region=us-east-1# Week 2 Achievements: Understood what AWS is and mastered the basic service groups:\nCompute Storage Networking Database \u0026hellip; Successfully created and configured an AWS Free Tier account.\nBecame familiar with the AWS Management Console and learned how to find, access, and use services via the web interface.\nInstalled and configured AWS CLI on the computer, including:\nAccess Key Secret Key Default Region \u0026hellip; Used AWS CLI to perform basic operations such as:\nCheck account \u0026amp; configuration information Retrieve the list of regions View EC2 service Create and manage key pairs Check information about running services \u0026hellip; Acquired the ability to connect between the web interface and CLI to manage AWS resources in parallel.\n\u0026hellip;\n"},{"uri":"https://thienluhoan.github.io/workshop-template/1-worklog/1.3-week3/","title":"Week 3 Worklog","tags":[],"description":"","content":"Week 3 Objectives: Conquer Amazon EC2: Understand how to initialize virtual servers, choose configuration (Instance types) and manage hard drives (EBS). Master Amazon VPC: Understand how to design virtual networks, subnet and control access flows. Connect infrastructure: Practice connecting Web Server (EC2) to Database (RDS) created in week 2. Tasks to be carried out this week: Day Task Start Date End Date Resource 2 - Compute \u0026amp; Storage Theory (Block): - Read about Amazon EC2: Instance types (T, M, C\u0026hellip;), Lifecycle. - Learn Amazon EBS: Volume types (GP3, IO2\u0026hellip;), how to mount volumes to EC2. - Learn about AMI (Amazon Machine Image). 23/09/2025 23/09/2025 https://explore.skillbuilder.aws/learn 3 - Complete lab and understand EC2 Lab goals: - Practice: + Create Key Pair and Security Group + Launch an EC2 Instance + SSH into server and install Web Server + ‚Ä¶ 24/09/2025 24/09/2025 https://us-east-1.console.aws.amazon.com/ec2/home?region=us-east-1 4 - Learn Networking Theory (VPC): - Read about VPC architecture: CIDR block, Subnet (Public vs Private). - Learn Internet Gateway (IGW) and Route Table. - Distinguish Security Group (Stateful) vs NACL (Stateless). 25/09/2025 25/09/2025 https://docs.aws.amazon.com/vpc/ 5 - Do VPC lab to design system network. - Read instructions and prepare steps to create Custom VPC. - Practice: + Create new VPC and divide Subnets + Attach Internet Gateway and configure Route Table + ‚Ä¶ 26/09/2025 26/09/2025 https://us-east-1.console.aws.amazon.com/vpc/home?region=us-east-1 6 - Integration Practice: + Connect Web Server (EC2) with Database (RDS) created last week + Configure Security Group to allow port 3306 + Complete basic 2-Tier architecture 27/09/2025 27/09/2025 https://us-east-1.console.aws.amazon.com/ec2/v2/home Week 3 Outcomes: Mastered foundational knowledge of Compute (EC2) and Networking (VPC). Completed labs on creating virtual servers and designing VPC networks. Learned how to use Security Groups to secure and control access between resources. Completed integration challenge: Successfully connected Web Server (EC2) and Database (RDS). Successfully built a basic 2-Tier architecture on AWS. \u0026hellip; "},{"uri":"https://thienluhoan.github.io/workshop-template/1-worklog/1.4-week4/","title":"Week 4 Worklog","tags":[],"description":"","content":"Week 4 Objectives: Complete the design of the homepage (Homepage UI). Creatively design the auxiliary pages: Policy \u0026amp; Returns, Shipping, FAQ. Plan the design for the product catalog (Product Catalog). Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Continue designing in Figma and learning additional topics. - Clean the web framework (remove errors, do a clean build). 29/09/2025 03/10/2025 https://www.figma.com/design/U72937c9sp4cD0O5Fcwlw0/AWS-2025?node-id=81-3129\u0026t=mlq1HOld0udU3Bk1-0 3 - Register and do the internship at the office. Learn and understand S3 to support the Back-End with data. - Learn and design Figma for the next day‚Äôs web design. 29/09/2025 03/10/2025 https://www.figma.com/design/U72937c9sp4cD0O5Fcwlw0/AWS-2025?node-id=81-3129\u0026t=mlq1HOld0udU3Bk1-0 4 - First step: create the Home page and optimize the Header and Footer. - Creatively design Policy \u0026amp; Returns, Shipping, etc., to build a Help/FAQ section for users. 29/09/2025 03/10/2025 https://www.figma.com/design/U72937c9sp4cD0O5Fcwlw0/AWS-2025?node-id=81-3129\u0026t=mlq1HOld0udU3Bk1-0 5 - Plan the web design based on the Figma and complete the HOMEPAGE UI. - Collect and save product images to add products to the website; discuss and plan pages/accounts for admin, staff, etc. 29/09/2025 03/10/2025 https://www.figma.com/design/U72937c9sp4cD0O5Fcwlw0/AWS-2025?node-id=81-3129\u0026t=mlq1HOld0udU3Bk1-0 6 - Continue the design and create site-wide footer items for the product catalog to bring products into the UI. - Plan weekend work and set completion goals. 29/09/2025 03/10/2025 https://www.figma.com/design/U72937c9sp4cD0O5Fcwlw0/AWS-2025?node-id=81-3129\u0026t=mlq1HOld0udU3Bk1-0 Week 4 Achievements: Continued developing design skills: drew more in Figma; cleaned and stabilized the web framework. Joined the internship; learned S3 to support Back-End data usage. Designed Figma sections (Policy \u0026amp; Returns, Shipping, etc.) to form a Help/FAQ area. Implemented the HOMEPAGE UI and planned admin/staff account pages. Finalized footer items and brought products into the UI; planned weekend work with clear goals. \u0026hellip; "},{"uri":"https://thienluhoan.github.io/workshop-template/1-worklog/1.5-week5/","title":"Week 5 Worklog","tags":[],"description":"","content":"Week 5 Objectives: Develop the Administration Subsystem (Admin \u0026amp; Staff Dashboard): Complete the Figma design and deploy the Front-End for the login pages, Admin and Staff management pages. Unify the Architecture with the Back-End: Unify the AWS services to be used and the communication process between FE and BE (API). Solve technical problems (Troubleshooting): Act as technical support, fix errors (blockers) that teammates encounter to ensure the overall progress. Prepare sample data (Mocking): Create sample product data on the interface while waiting for the Back-End to complete the API. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Completed the weekend plan: added the product section interface in the footer and designed Figma screens for admin and staff. - Same day: office registration was unsuccessful, so continued Front-End work. 06/10/2025 10/10/2025 https://github.com/khanhtm45/AWS/tree/main/frontend https://github.com/khanhtm45/AWS/tree/main/backend 3 - Worked with teammates on login and page designs for admin and staff; fixed blockers teammates couldn‚Äôt resolve. - Completed login testing for admin/staff and designed their interfaces. 06/10/2025 10/10/2025 https://github.com/khanhtm45/AWS/tree/main/frontend https://github.com/khanhtm45/AWS/tree/main/backend 4 - Met with Back-End (BE) to align on appropriate AWS services for the project. - Handed over assigned tasks; the team reviewed for gaps/overlaps and provided feedback to proceed with next interfaces. 06/10/2025 10/10/2025 https://github.com/khanhtm45/AWS/tree/main/frontend https://github.com/khanhtm45/AWS/tree/main/backend 5 - Added product images and demo product details while waiting for BE to finish API \u0026amp; Database. - Completed login tests and finalized admin/staff interfaces. 06/10/2025 10/10/2025 https://github.com/khanhtm45/AWS/tree/main/frontend https://github.com/khanhtm45/AWS/tree/main/backend 6 - Followed up with teammates to complete leader-assigned tasks. - Translated blog posts as a side task requested by advisors; delivered in .docx, minimizing code. 06/10/2025 10/10/2025 https://github.com/khanhtm45/AWS/tree/main/frontend https://github.com/khanhtm45/AWS/tree/main/backend Week 5 Achievements: Completed footer product section and Figma designs for admin/staff; continued Front-End when office registration didn‚Äôt go through. Built and tested login flows; fixed blockers for teammates; finalized admin/staff interfaces. Synced with Back-End on AWS service choices; performed team handover/review for next UI steps. Prepared data and visuals (product images, demo details) while waiting for API/DB completion. Supported the team: followed up on tasks; translated advisor-requested blog posts into .docx with minimal code. \u0026hellip; "},{"uri":"https://thienluhoan.github.io/workshop-template/1-worklog/1.6-week6/","title":"Week 6 Worklog","tags":[],"description":"","content":"Week 6 Objectives: Prepare lessons and study for exams. Draw the structure to send to the Mentor for check. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Plan to prepare for the exam, prepare questions to study and review knowledge. - Read carefully the review content that the mentors sent and stick to the exam questions. 13/10/2025 15/10/2025 https://www.notion.so/Nguy-n-Duy-Hi-u-26b17fef5a7781bf8f71e6846c27ad86 3 - Continue studying for the exam and take a day to review the practical knowledge and labs. 13/10/2025 15/10/2025 https://www.notion.so/Nguy-n-Duy-Hi-u-26b17fef5a7781bf8f71e6846c27ad86 4 - Find information and review exercises that you can use to review for your upcoming midterms. - Looking for exam questions to follow and continuing the unfinished project. 13/10/2025 15/10/2025 https://www.notion.so/Nguy-n-Duy-Hi-u-26b17fef5a7781bf8f71e6846c27ad86 5 - Meet and support team members on project architecture drawing to apply aws technical design to the project. - Edit the drawing structure, research and the whole team works together to perfect the structure so that it can be submitted to the mentors to check if it is okay and can be applied to the project. 16/10/2025 18/10/2025 https://skillbuilder.aws/ 6 - But the project failed due to drawing errors. And using icons that were sleeping and other reasons that were not suitable for project management costs, so the brothers suggested that it should be removed and the aws application structure should be redrawn. - Meet and re-plan to prepare for a more complete redraw of the structure. 16/10/2025 18/10/2025 https://cloudjourney.awsstudygroup.com/ Week 6 Achievements: Plan, prepare questions, and stick to the review content from mentors to prepare for the exam.\nContinue studying for the exam and spend a day reviewing practical knowledge and labs.\nFind materials and exercises for the midterm review and continue the unfinished project.\nMeet with the team, draw, and finalize the project\u0026rsquo;s AWS architecture structure for mentor approval.\nThe project architecture failed due to errors and costs; the team met to plan a redraw.\n\u0026hellip;\n"},{"uri":"https://thienluhoan.github.io/workshop-template/1-worklog/1.7-week7/","title":"Week 7 Worklog","tags":[],"description":"","content":"Week 7 Objectives: Draw, design and plan your studies. Review for midterm exams. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Continue drawing and planning to prepare for the AI ‚Äã‚Äãchat box design consulting for the homepage. - Learn about Powered by AWS Bedrock AI and how to prepare it for your project on the homepage. 20/10/2025 20/10/2025 https://aws.amazon.com/vi/bedrock/?trk=58fd0fb1-df1c-4e26-b990-06d09d890997\u0026sc_channel=ps\u0026ef_id=Cj0KCQjwsPzHBhDCARIsALlWNG0lISmbTGhdjL825lNO1w-a8ez9OXwlmCc5XsSTNZGIDtgVpchCUtgaAvkOEALw_wcB:G:s\u0026s_kwcid=AL!4422!3!692062112144!p!!g!!bedrock!21054970946!157173566857\u0026gad_campaignid=21054970946\u0026gbraid=0AAAAADjHtp8Si2C_9mrOH-lEBa26rs-CT\u0026gclid=Cj0KCQjwsPzHBhDCARIsALlWNG0lISmbTGhdjL825lNO1w-a8ez9OXwlmCc5XsSTNZGIDtgVpchCUtgaAvkOEALw_wcB 3 - Decide to use a chat infrastructure powered by AWS Bedrock AI. - Write code and debug to ensure the chat box works smoothly. 21/10/2025 23/10/2025 https://aws.amazon.com/vi/bedrock/?trk=58fd0fb1-df1c-4e26-b990-06d09d890997\u0026sc_channel=ps\u0026ef_id=Cj0KCQjwsPzHBhDCARIsALlWNG0lISmbTGhdjL825lNO1w-a8ez9OXwlmCc5XsSTNZGIDtgVpchCUtgaAvkOEALw_wcB:G:s\u0026s_kwcid=AL!4422!3!692062112144!p!!g!!bedrock!21054970946!157173566857\u0026gad_campaignid=21054970946\u0026gbraid=0AAAAADjHtp8Si2C_9mrOH-lEBa26rs-CT\u0026gclid=Cj0KCQjwsPzHBhDCARIsALlWNG0lISmbTGhdjL825lNO1w-a8ez9OXwlmCc5XsSTNZGIDtgVpchCUtgaAvkOEALw_wcB 4 - After coding, with the support of AI, the chat box part can be basically completed. - Complete the interface design and discuss with BE so we can continue working on the inside of the chat box. 21/10/2025 23/10/2025 https://aws.amazon.com/vi/bedrock/?trk=58fd0fb1-df1c-4e26-b990-06d09d890997\u0026sc_channel=ps\u0026ef_id=Cj0KCQjwsPzHBhDCARIsALlWNG0lISmbTGhdjL825lNO1w-a8ez9OXwlmCc5XsSTNZGIDtgVpchCUtgaAvkOEALw_wcB:G:s\u0026s_kwcid=AL!4422!3!692062112144!p!!g!!bedrock!21054970946!157173566857\u0026gad_campaignid=21054970946\u0026gbraid=0AAAAADjHtp8Si2C_9mrOH-lEBa26rs-CT\u0026gclid=Cj0KCQjwsPzHBhDCARIsALlWNG0lISmbTGhdjL825lNO1w-a8ez9OXwlmCc5XsSTNZGIDtgVpchCUtgaAvkOEALw_wcB 5 -BE is assigning and waiting for assigned tasks. - Finalize the initial structure of the interface and team members are testing the features for bugs. 21/10/2025 23/10/2025 https://aws.amazon.com/vi/bedrock/?trk=58fd0fb1-df1c-4e26-b990-06d09d890997\u0026sc_channel=ps\u0026ef_id=Cj0KCQjwsPzHBhDCARIsALlWNG0lISmbTGhdjL825lNO1w-a8ez9OXwlmCc5XsSTNZGIDtgVpchCUtgaAvkOEALw_wcB:G:s\u0026s_kwcid=AL!4422!3!692062112144!p!!g!!bedrock!21054970946!157173566857\u0026gad_campaignid=21054970946\u0026gbraid=0AAAAADjHtp8Si2C_9mrOH-lEBa26rs-CT\u0026gclid=Cj0KCQjwsPzHBhDCARIsALlWNG0lISmbTGhdjL825lNO1w-a8ez9OXwlmCc5XsSTNZGIDtgVpchCUtgaAvkOEALw_wcB 6 - Study for midterms, do labs again so you can apply the definitions, learn theory that can be given in midterms. - Finalize the initial structure of the interface and team members are testing the features for bugs. 21/10/2025 23/10/2025 https://skillbuilder.aws/ Week 7 Achievements: Sketch and plan the AI chat box design consultation for the homepage.\nResearch ‚ÄúPowered by AWS Bedrock AI‚Äù and prepare integration for the homepage project.\nDecide to adopt a chat infrastructure powered by AWS Bedrock AI.\nImplement and debug the chat box to ensure smooth operation.\nBasically complete the chat box with AI support; finalize the UI and align with BE for internal logic.\nBE assigns tasks; finalize the initial UI structure, team tests features for bugs; study for midterms and redo labs.\n\u0026hellip;\n"},{"uri":"https://thienluhoan.github.io/workshop-template/1-worklog/1.8-week8/","title":"Week 8 Worklog","tags":[],"description":"","content":"Week 8 Objectives: Study your lessons and theory for the midterm exam. Midterm exam. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Go to the office to study. - RStudy with mentors to prepare for midterm exams. 27/10/2025 31/10/2025 https://skillbuilder.aws/ 3 - Review your lessons with quizzes on quizlet. - Thorough study of solution architect theory. 27/10/2025 31/10/2025 https://quizlet.com/340744042/aws-flash-cards/?funnelUUID=62f88ee4-b28f-4699-b909-a959d8bd4efd 4 - View sample and code of SAA exam. - Use AI chat to create self-reading questions and choose test answers like an exam. 27/10/2025 31/10/2025 https://ezse.net/aws/certified-cloud-practitioner/cau-hoi-co-ban.html 5 - Learn the theory and application of aws for solutions. - Review the theory to see which aws applications are suitable for cost saving, dangerous conditions,\u0026hellip;.. 27/10/2025 31/10/2025 https://skillbuilder.aws/ 6 - Midterm exam. 31/10/2025 31/10/2025 Week 8 Achievements: Come to the office to study and review with a mentor to prepare for the midterm exam.\nReview with Quizlet question sets.\nLearn in-depth Solution Architect theory.\nView sample/code snippets of SAA; use AI to generate self-practice questions and choose answers like the real exam.\nLearn AWS theory \u0026amp; application for solutions; review appropriate options to save costs and handle risky situations.\nMidterm exam (October 31, 2025).\n\u0026hellip;\n"},{"uri":"https://thienluhoan.github.io/workshop-template/1-worklog/1.9-week9/","title":"Week 9 Worklog","tags":[],"description":"","content":"Week 9 Objectives: Setup a parallel running environment (Frontend - Backend - Docker) to develop the project. Complete the integration of Product Management and Category APIs. Solve the challenge of displaying images from AWS S3. Tasks to be carried out this week: Day Task Start Date Completion Date Resource 2 - Integrate Backend API and test with Swagger before implementing API into Frontend. - Check Login functions to configure and add admin permissions. 03/11/2025 09/11/2025 http://localhost:8080/swagger-ui/index.html?gidzl=Un-z6C7ELdHx1huVmjDLSde1l1xRqL1wPbRiGjxKLtfc0EyGqOG4AMm1kKJItmKiFrplGZUFvjbSnyrKSW\u0026continue=#/product-variant-controller/listProductVariants 3 - Design Admin UI for Categories and Product Management. - Integrate Product Management and Category APIs. - Troubleshoot Backend issues regarding installation and synchronization with Docker. 03/11/2025 09/11/2025 http://localhost:8080/swagger-ui/index.html?gidzl=Un-z6C7ELdHx1huVmjDLSde1l1xRqL1wPbRiGjxKLtfc0EyGqOG4AMm1kKJItmKiFrplGZUFvjbSnyrKSW\u0026continue=#/product-variant-controller/listProductVariants 4 - Setup environment and link Backend Git. - Run the system in parallel between Backend, Docker, and Frontend. - Add new APIs and check system functionality. 03/11/2025 09/11/2025 http://localhost:8080/swagger-ui/index.html?gidzl=Un-z6C7ELdHx1huVmjDLSde1l1xRqL1wPbRiGjxKLtfc0EyGqOG4AMm1kKJItmKiFrplGZUFvjbSnyrKSW\u0026continue=#/product-variant-controller/listProductVariants 5 - Complete API integration for both sides. - AWS S3 Challenge: Handle logic to fetch images from S3 and display them on the Web. - Learn and research how to retrieve S3 Keys for display. - Find a way to add, edit, and delete displayed images via product categories using API. 03/11/2025 09/11/2025 http://localhost:8080/swagger-ui/index.html?gidzl=Un-z6C7ELdHx1huVmjDLSde1l1xRqL1wPbRiGjxKLtfc0EyGqOG4AMm1kKJItmKiFrplGZUFvjbSnyrKSW\u0026continue=#/product-variant-controller/listProductVariants 6 - Finalize Features: + Add, edit, delete products (CRUD) + Display the complete product list in the project management section. 03/11/2025 09/11/2025 http://localhost:8080/swagger-ui/index.html?gidzl=Un-z6C7ELdHx1huVmjDLSde1l1xRqL1wPbRiGjxKLtfc0EyGqOG4AMm1kKJItmKiFrplGZUFvjbSnyrKSW\u0026continue=#/product-variant-controller/listProductVariants Week 9 Achievements: Synchronized Development Environment Setup:\nSuccessfully installed and configured a parallel running environment between Backend, Docker, and Frontend. Completed Backend Git linking, ensuring source code synchronization. API Integration and Authorization:\nUnderstood and proficiently used Swagger to test APIs before integrating into Frontend. Successfully handled Login flow and Admin permission assignment. Product Management Module Completion:\nCompleted UI design for Product Management and Categories pages. Successfully integrated APIs for full CRUD functions (Create, Read, Update, Delete) for products. Solved Image Storage Challenge with AWS S3:\nGrasped the object storage mechanism on S3. Resolved the logic issue regarding retrieving S3 Keys from Backend and displaying product images on the Frontend interface. \u0026hellip; "},{"uri":"https://thienluhoan.github.io/workshop-template/1-worklog/","title":"Worklog","tags":[],"description":"","content":"On this page, you will need to introduce your worklog. How did you complete it? How many weeks did you take to complete the program? What did you do in those weeks?\nTypically, and as a standard, a worklog is carried out over about 3 months (throughout the internship period) with weekly contents as follows:\nWeek 1: Getting familiar with AWS and basic AWS services\nWeek 2: Understanding AWS Cloud9, S3, RDS basics and completing Labs\nWeek 3: Mastering Amazon EC2, VPC and connecting Web Server infrastructure with Database\nWeek 4: Completing homepage design and supporting pages on Figma\nWeek 5: Developing Admin Dashboard and consolidating architecture with Backend\nWeek 6: Reviewing and preparing for exams, drawing project structure\nWeek 7: Designing and integrating AI chatbox using AWS Bedrock\nWeek 8: Studying theory and taking midterm exam\nWeek 9: Setting up environment and completing API integration for product management\nWeek 10: Integrating APIs into frontend and practicing AWS Workshop\nWeek 11: Completing basic project and preparing for submission\nWeek 12: Reviewing system, preparing demo and presentation materials\n"},{"uri":"https://thienluhoan.github.io/workshop-template/5-workshop/5.2-prerequiste/","title":"Prerequisites","tags":[],"description":"","content":"IAM Permissions Before starting this workshop, you need to create an IAM role with the following permissions:\nNavigate to IAM Console Create a new IAM role for Lambda Attach the following policies: AmazonBedrockFullAccess - Required to invoke Bedrock models CloudWatchLogsFullAccess - Required for Lambda logging AWS Account Requirements Active AWS Account Access to Amazon Bedrock (available in us-east-1, us-west-2, or other supported regions) Permissions to create Lambda functions, API Gateway, and S3 buckets Knowledge Prerequisites Basic understanding of:\nAWS Lambda functions REST APIs JavaScript/Node.js HTML/CSS basics Let\u0026rsquo;s get started with setting up Amazon Bedrock!\n{\r\u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;,\r\u0026#34;Statement\u0026#34;: [\r{\r\u0026#34;Sid\u0026#34;: \u0026#34;VisualEditor0\u0026#34;,\r\u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;,\r\u0026#34;Action\u0026#34;: [\r\u0026#34;cloudformation:*\u0026#34;,\r\u0026#34;cloudwatch:*\u0026#34;,\r\u0026#34;ec2:AcceptTransitGatewayPeeringAttachment\u0026#34;,\r\u0026#34;ec2:AcceptTransitGatewayVpcAttachment\u0026#34;,\r\u0026#34;ec2:AllocateAddress\u0026#34;,\r\u0026#34;ec2:AssociateAddress\u0026#34;,\r\u0026#34;ec2:AssociateIamInstanceProfile\u0026#34;,\r\u0026#34;ec2:AssociateRouteTable\u0026#34;,\r\u0026#34;ec2:AssociateSubnetCidrBlock\u0026#34;,\r\u0026#34;ec2:AssociateTransitGatewayRouteTable\u0026#34;,\r\u0026#34;ec2:AssociateVpcCidrBlock\u0026#34;,\r\u0026#34;ec2:AttachInternetGateway\u0026#34;,\r\u0026#34;ec2:AttachNetworkInterface\u0026#34;,\r\u0026#34;ec2:AttachVolume\u0026#34;,\r\u0026#34;ec2:AttachVpnGateway\u0026#34;,\r\u0026#34;ec2:AuthorizeSecurityGroupEgress\u0026#34;,\r\u0026#34;ec2:AuthorizeSecurityGroupIngress\u0026#34;,\r\u0026#34;ec2:CreateClientVpnEndpoint\u0026#34;,\r\u0026#34;ec2:CreateClientVpnRoute\u0026#34;,\r\u0026#34;ec2:CreateCustomerGateway\u0026#34;,\r\u0026#34;ec2:CreateDhcpOptions\u0026#34;,\r\u0026#34;ec2:CreateFlowLogs\u0026#34;,\r\u0026#34;ec2:CreateInternetGateway\u0026#34;,\r\u0026#34;ec2:CreateLaunchTemplate\u0026#34;,\r\u0026#34;ec2:CreateNetworkAcl\u0026#34;,\r\u0026#34;ec2:CreateNetworkInterface\u0026#34;,\r\u0026#34;ec2:CreateNetworkInterfacePermission\u0026#34;,\r\u0026#34;ec2:CreateRoute\u0026#34;,\r\u0026#34;ec2:CreateRouteTable\u0026#34;,\r\u0026#34;ec2:CreateSecurityGroup\u0026#34;,\r\u0026#34;ec2:CreateSubnet\u0026#34;,\r\u0026#34;ec2:CreateSubnetCidrReservation\u0026#34;,\r\u0026#34;ec2:CreateTags\u0026#34;,\r\u0026#34;ec2:CreateTransitGateway\u0026#34;,\r\u0026#34;ec2:CreateTransitGatewayPeeringAttachment\u0026#34;,\r\u0026#34;ec2:CreateTransitGatewayPrefixListReference\u0026#34;,\r\u0026#34;ec2:CreateTransitGatewayRoute\u0026#34;,\r\u0026#34;ec2:CreateTransitGatewayRouteTable\u0026#34;,\r\u0026#34;ec2:CreateTransitGatewayVpcAttachment\u0026#34;,\r\u0026#34;ec2:CreateVpc\u0026#34;,\r\u0026#34;ec2:CreateVpcEndpoint\u0026#34;,\r\u0026#34;ec2:CreateVpcEndpointConnectionNotification\u0026#34;,\r\u0026#34;ec2:CreateVpcEndpointServiceConfiguration\u0026#34;,\r\u0026#34;ec2:CreateVpnConnection\u0026#34;,\r\u0026#34;ec2:CreateVpnConnectionRoute\u0026#34;,\r\u0026#34;ec2:CreateVpnGateway\u0026#34;,\r\u0026#34;ec2:DeleteCustomerGateway\u0026#34;,\r\u0026#34;ec2:DeleteFlowLogs\u0026#34;,\r\u0026#34;ec2:DeleteInternetGateway\u0026#34;,\r\u0026#34;ec2:DeleteNetworkInterface\u0026#34;,\r\u0026#34;ec2:DeleteNetworkInterfacePermission\u0026#34;,\r\u0026#34;ec2:DeleteRoute\u0026#34;,\r\u0026#34;ec2:DeleteRouteTable\u0026#34;,\r\u0026#34;ec2:DeleteSecurityGroup\u0026#34;,\r\u0026#34;ec2:DeleteSubnet\u0026#34;,\r\u0026#34;ec2:DeleteSubnetCidrReservation\u0026#34;,\r\u0026#34;ec2:DeleteTags\u0026#34;,\r\u0026#34;ec2:DeleteTransitGateway\u0026#34;,\r\u0026#34;ec2:DeleteTransitGatewayPeeringAttachment\u0026#34;,\r\u0026#34;ec2:DeleteTransitGatewayPrefixListReference\u0026#34;,\r\u0026#34;ec2:DeleteTransitGatewayRoute\u0026#34;,\r\u0026#34;ec2:DeleteTransitGatewayRouteTable\u0026#34;,\r\u0026#34;ec2:DeleteTransitGatewayVpcAttachment\u0026#34;,\r\u0026#34;ec2:DeleteVpc\u0026#34;,\r\u0026#34;ec2:DeleteVpcEndpoints\u0026#34;,\r\u0026#34;ec2:DeleteVpcEndpointServiceConfigurations\u0026#34;,\r\u0026#34;ec2:DeleteVpnConnection\u0026#34;,\r\u0026#34;ec2:DeleteVpnConnectionRoute\u0026#34;,\r\u0026#34;ec2:Describe*\u0026#34;,\r\u0026#34;ec2:DetachInternetGateway\u0026#34;,\r\u0026#34;ec2:DisassociateAddress\u0026#34;,\r\u0026#34;ec2:DisassociateRouteTable\u0026#34;,\r\u0026#34;ec2:GetLaunchTemplateData\u0026#34;,\r\u0026#34;ec2:GetTransitGatewayAttachmentPropagations\u0026#34;,\r\u0026#34;ec2:ModifyInstanceAttribute\u0026#34;,\r\u0026#34;ec2:ModifySecurityGroupRules\u0026#34;,\r\u0026#34;ec2:ModifyTransitGatewayVpcAttachment\u0026#34;,\r\u0026#34;ec2:ModifyVpcAttribute\u0026#34;,\r\u0026#34;ec2:ModifyVpcEndpoint\u0026#34;,\r\u0026#34;ec2:ReleaseAddress\u0026#34;,\r\u0026#34;ec2:ReplaceRoute\u0026#34;,\r\u0026#34;ec2:RevokeSecurityGroupEgress\u0026#34;,\r\u0026#34;ec2:RevokeSecurityGroupIngress\u0026#34;,\r\u0026#34;ec2:RunInstances\u0026#34;,\r\u0026#34;ec2:StartInstances\u0026#34;,\r\u0026#34;ec2:StopInstances\u0026#34;,\r\u0026#34;ec2:UpdateSecurityGroupRuleDescriptionsEgress\u0026#34;,\r\u0026#34;ec2:UpdateSecurityGroupRuleDescriptionsIngress\u0026#34;,\r\u0026#34;iam:AddRoleToInstanceProfile\u0026#34;,\r\u0026#34;iam:AttachRolePolicy\u0026#34;,\r\u0026#34;iam:CreateInstanceProfile\u0026#34;,\r\u0026#34;iam:CreatePolicy\u0026#34;,\r\u0026#34;iam:CreateRole\u0026#34;,\r\u0026#34;iam:DeleteInstanceProfile\u0026#34;,\r\u0026#34;iam:DeletePolicy\u0026#34;,\r\u0026#34;iam:DeleteRole\u0026#34;,\r\u0026#34;iam:DeleteRolePolicy\u0026#34;,\r\u0026#34;iam:DetachRolePolicy\u0026#34;,\r\u0026#34;iam:GetInstanceProfile\u0026#34;,\r\u0026#34;iam:GetPolicy\u0026#34;,\r\u0026#34;iam:GetRole\u0026#34;,\r\u0026#34;iam:GetRolePolicy\u0026#34;,\r\u0026#34;iam:ListPolicyVersions\u0026#34;,\r\u0026#34;iam:ListRoles\u0026#34;,\r\u0026#34;iam:PassRole\u0026#34;,\r\u0026#34;iam:PutRolePolicy\u0026#34;,\r\u0026#34;iam:RemoveRoleFromInstanceProfile\u0026#34;,\r\u0026#34;lambda:CreateFunction\u0026#34;,\r\u0026#34;lambda:DeleteFunction\u0026#34;,\r\u0026#34;lambda:DeleteLayerVersion\u0026#34;,\r\u0026#34;lambda:GetFunction\u0026#34;,\r\u0026#34;lambda:GetLayerVersion\u0026#34;,\r\u0026#34;lambda:InvokeFunction\u0026#34;,\r\u0026#34;lambda:PublishLayerVersion\u0026#34;,\r\u0026#34;logs:CreateLogGroup\u0026#34;,\r\u0026#34;logs:DeleteLogGroup\u0026#34;,\r\u0026#34;logs:DescribeLogGroups\u0026#34;,\r\u0026#34;logs:PutRetentionPolicy\u0026#34;,\r\u0026#34;route53:ChangeTagsForResource\u0026#34;,\r\u0026#34;route53:CreateHealthCheck\u0026#34;,\r\u0026#34;route53:CreateHostedZone\u0026#34;,\r\u0026#34;route53:CreateTrafficPolicy\u0026#34;,\r\u0026#34;route53:DeleteHostedZone\u0026#34;,\r\u0026#34;route53:DisassociateVPCFromHostedZone\u0026#34;,\r\u0026#34;route53:GetHostedZone\u0026#34;,\r\u0026#34;route53:ListHostedZones\u0026#34;,\r\u0026#34;route53domains:ListDomains\u0026#34;,\r\u0026#34;route53domains:ListOperations\u0026#34;,\r\u0026#34;route53domains:ListTagsForDomain\u0026#34;,\r\u0026#34;route53resolver:AssociateResolverEndpointIpAddress\u0026#34;,\r\u0026#34;route53resolver:AssociateResolverRule\u0026#34;,\r\u0026#34;route53resolver:CreateResolverEndpoint\u0026#34;,\r\u0026#34;route53resolver:CreateResolverRule\u0026#34;,\r\u0026#34;route53resolver:DeleteResolverEndpoint\u0026#34;,\r\u0026#34;route53resolver:DeleteResolverRule\u0026#34;,\r\u0026#34;route53resolver:DisassociateResolverEndpointIpAddress\u0026#34;,\r\u0026#34;route53resolver:DisassociateResolverRule\u0026#34;,\r\u0026#34;route53resolver:GetResolverEndpoint\u0026#34;,\r\u0026#34;route53resolver:GetResolverRule\u0026#34;,\r\u0026#34;route53resolver:ListResolverEndpointIpAddresses\u0026#34;,\r\u0026#34;route53resolver:ListResolverEndpoints\u0026#34;,\r\u0026#34;route53resolver:ListResolverRuleAssociations\u0026#34;,\r\u0026#34;route53resolver:ListResolverRules\u0026#34;,\r\u0026#34;route53resolver:ListTagsForResource\u0026#34;,\r\u0026#34;route53resolver:UpdateResolverEndpoint\u0026#34;,\r\u0026#34;route53resolver:UpdateResolverRule\u0026#34;,\r\u0026#34;s3:AbortMultipartUpload\u0026#34;,\r\u0026#34;s3:CreateBucket\u0026#34;,\r\u0026#34;s3:DeleteBucket\u0026#34;,\r\u0026#34;s3:DeleteObject\u0026#34;,\r\u0026#34;s3:GetAccountPublicAccessBlock\u0026#34;,\r\u0026#34;s3:GetBucketAcl\u0026#34;,\r\u0026#34;s3:GetBucketOwnershipControls\u0026#34;,\r\u0026#34;s3:GetBucketPolicy\u0026#34;,\r\u0026#34;s3:GetBucketPolicyStatus\u0026#34;,\r\u0026#34;s3:GetBucketPublicAccessBlock\u0026#34;,\r\u0026#34;s3:GetObject\u0026#34;,\r\u0026#34;s3:GetObjectVersion\u0026#34;,\r\u0026#34;s3:GetBucketVersioning\u0026#34;,\r\u0026#34;s3:ListAccessPoints\u0026#34;,\r\u0026#34;s3:ListAccessPointsForObjectLambda\u0026#34;,\r\u0026#34;s3:ListAllMyBuckets\u0026#34;,\r\u0026#34;s3:ListBucket\u0026#34;,\r\u0026#34;s3:ListBucketMultipartUploads\u0026#34;,\r\u0026#34;s3:ListBucketVersions\u0026#34;,\r\u0026#34;s3:ListJobs\u0026#34;,\r\u0026#34;s3:ListMultipartUploadParts\u0026#34;,\r\u0026#34;s3:ListMultiRegionAccessPoints\u0026#34;,\r\u0026#34;s3:ListStorageLensConfigurations\u0026#34;,\r\u0026#34;s3:PutAccountPublicAccessBlock\u0026#34;,\r\u0026#34;s3:PutBucketAcl\u0026#34;,\r\u0026#34;s3:PutBucketPolicy\u0026#34;,\r\u0026#34;s3:PutBucketPublicAccessBlock\u0026#34;,\r\u0026#34;s3:PutObject\u0026#34;,\r\u0026#34;secretsmanager:CreateSecret\u0026#34;,\r\u0026#34;secretsmanager:DeleteSecret\u0026#34;,\r\u0026#34;secretsmanager:DescribeSecret\u0026#34;,\r\u0026#34;secretsmanager:GetSecretValue\u0026#34;,\r\u0026#34;secretsmanager:ListSecrets\u0026#34;,\r\u0026#34;secretsmanager:ListSecretVersionIds\u0026#34;,\r\u0026#34;secretsmanager:PutResourcePolicy\u0026#34;,\r\u0026#34;secretsmanager:TagResource\u0026#34;,\r\u0026#34;secretsmanager:UpdateSecret\u0026#34;,\r\u0026#34;sns:ListTopics\u0026#34;,\r\u0026#34;ssm:DescribeInstanceProperties\u0026#34;,\r\u0026#34;ssm:DescribeSessions\u0026#34;,\r\u0026#34;ssm:GetConnectionStatus\u0026#34;,\r\u0026#34;ssm:GetParameters\u0026#34;,\r\u0026#34;ssm:ListAssociations\u0026#34;,\r\u0026#34;ssm:ResumeSession\u0026#34;,\r\u0026#34;ssm:StartSession\u0026#34;,\r\u0026#34;ssm:TerminateSession\u0026#34;\r],\r\u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34;\r}\r]\r} Provision resources using CloudFormation In this lab, we will use N.Virginia region (us-east-1).\nTo prepare the workshop environment, deploy this CloudFormation Template (click link): PrivateLinkWorkshop . Accept all of the defaults when deploying the template.\nTick 2 acknowledgement boxes Choose Create stack The ClouddFormation deployment requires about 15 minutes to complete.\n2 VPCs have been created 3 EC2s have been created "},{"uri":"https://thienluhoan.github.io/workshop-template/2-proposal/","title":"Proposal","tags":[],"description":"","content":"\u0026ldquo;Leaf\u0026rdquo; Clothing E-commerce Website Using AWS Services Integration to Optimize the System (S3, CloudWatch, etc.) 1. Executive Summary Leaf is an e-commerce platform specializing in fashion products for men and women, including fashion accessories such as jewelry, shoes, hats, and leather straps. The website integrates AWS services to optimize costs and enhance the user experience.\n2. Problem Statement Problem:\nA clothing store needs its own e-commerce channel to optimize the user experience.\nSolution:\nCreate a dedicated e-commerce website for the store using AWS services to optimize cost, time, and user experience.\n3. Solution Architecture The platform applies an AWS Serverless architecture to manage data.\nComponents and Roles in the AWS Architecture A. User Interface Layer Service Role Detailed Description AWS Amplify Website deployment Hosts static websites (React, Vue, Next.js) and automatically builds/deploys when code is pushed to GitHub. Amazon CloudFront (CDN) Improve page loading speed Caches static content (images, CSS, JS) close to users to reduce latency and bandwidth usage. Amazon S3 Store static files \u0026amp; product images Acts as a content repository for images, banners, CSS/JS files. B. Application Logic Layer Service Role Detailed Description Amazon API Gateway API gateway Receives requests from frontend and forwards them to Lambda functions for processing. AWS Lambda Server-side logic Handles order processing, payments, authentication, email sending, without dedicated servers. Amazon DynamoDB NoSQL database Stores products, accounts, orders, shopping carts, offering high speed and automatic scaling. Amazon OpenSearch Service Product search Allows users to quickly search products by keywords, color, price, etc. Amazon EventBridge Event handling Automatically triggers events (e.g., new order ‚Üí send email, update stock). AWS Secrets Manager Secure sensitive data Stores API keys, payment tokens, database passwords to protect data. C. User Management \u0026amp; Security Layer Service Role Detailed Description Amazon Cognito User authentication \u0026amp; management Supports signup, login, password reset, MFA without building your own auth system. AWS WAF (Web Application Firewall) Web protection Protects against SQL Injection, XSS, DDoS, and other malicious access. Amazon Route 53 DNS \u0026amp; domain Manages domain names. D. Notification \u0026amp; Communication Layer Service Role Detailed Description Amazon SNS (Simple Notification Service) System notifications Sends notifications to admins or users (via email, SMS, or push notifications). Amazon SES (Simple Email Service) Transactional emails Sends order confirmations, promotions, password reset emails, etc. E. AI \u0026amp; Machine Learning Layer Service Role Detailed Description Amazon Translate Content translation Translates product descriptions to other languages for international customers. Amazon Bedrock AI content generation Creates chatbots for shopping assistance. F. Monitoring \u0026amp; Management Layer Service Role Detailed Description Amazon CloudWatch System monitoring Monitors logs, performance, alerts for errors or cost spikes. AWS CloudTrail Administrative logging Tracks configuration changes (who changed what, and when) for auditing purposes. 4. Technical Implementation Implementation Stages:\nCollect system requirements and features Estimate cost and check feasibility Design UI prototypes using Figma Build database schema Develop frontend interface Build API, backend, integrate AWS services Test, deploy, and finalize the project 5. Timeline \u0026amp; Milestones Month 1: Learn AWS Month 2: Design and implement the project Month 3: Deployment and testing 6. Budget Estimate Check cost here: AWS Pricing Calculator\nStorage \u0026amp; Data Services Service Function Estimated Usage Cost/Month (USD) Notes Amazon S3 Store images, CSS, JS 10 GB storage, ~5k GET, ~500 PUT 0.35 Low data, low traffic DynamoDB Store orders \u0026amp; carts ~1 GB data, 100k read/write 0.20 On-demand mode OpenSearch Service Product search 1 small instance, 10% uptime 3.00 Reduced configuration due to small dataset Backend \u0026amp; Logic Processing Service Function Estimated Usage Cost/Month (USD) Notes AWS Lambda API processing, payments 100k requests, 128MB, 100ms 0.20 Very cheap due to serverless API Gateway API access 100k requests 0.10 Directly connected to Lambda Secrets Manager Secure API keys, DB 1 secret 0.40 Maintained EventBridge Internal event triggers 1k events/month 0.05 Lightweight for notifications/orders User Interface, Authentication \u0026amp; Security Service Function Estimated Usage Cost/Month (USD) Notes Amplify Hosting Frontend deployment 10 GB, 3 builds/month 1.50 CI/CD + static hosting CloudFront (CDN) Content delivery 5 GB out 0.20 Reduce CDN cost WAF (Web Firewall) Web protection 1 ACL 5.00 Basic security required Cognito User authentication 100 MAU 1.00 Reduced from $5 baseline Route 53 Domain DNS 1 hosted zone 0.50 Unchanged Email \u0026amp; Notifications Service Function Estimated Usage Cost/Month (USD) Notes SES (Email) Order confirmation emails 1,000 emails/month 0.15 $0.0001 per email SNS (Notifications) HTTP/email notifications 1k messages 0.05 Used for internal notifications AI \u0026amp; Machine Learning (Optional) Service Function Estimated Usage Cost/Month (USD) Notes Translate Product translation EN‚ÜîVI 10k characters 0.15 Support international customers Bedrock Generate product descriptions 100 small requests 0.10 Can be disabled if not needed Monitoring \u0026amp; Logging Service Function Estimated Usage Cost/Month (USD) Notes CloudWatch Logs, metrics monitoring 1‚Äì2 GB log 1.50 Reduced from $9 CloudTrail Activity auditing Default usage 0.00 Free tier sufficient Total Estimated Monthly Cost Service Group Total Cost (USD/month) Storage \u0026amp; Data 3.55 Backend \u0026amp; Processing 0.75 UI \u0026amp; Security 8.20 Email \u0026amp; Notifications 0.20 AI \u0026amp; ML (Optional) 0.25 Monitoring \u0026amp; Logs 1.50 Total (Actual) ‚âà 14.45 USD / month (~375,000 VND) 7. Expected Outcomes The website runs with low latency and smooth image display. "},{"uri":"https://thienluhoan.github.io/workshop-template/1-worklog/1.10-week10/","title":"Week 10 Worklog","tags":[],"description":"","content":"Week 10 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Attach APIs piece by piece to the frontend and use F12 (DevTools) to check, remove, and minimize minor bugs. - Hold a meeting to choose a time for an offline session. 10/11/2025 16/11/2025 http://localhost:3001/ 3 - Meet offline at a caf√© to discuss the project more clearly and support the BE to add necessary features. - Continue focusing on the project and complete the remaining APIs. 10/11/2025 16/11/2025 http://localhost:3001/ 4 - Add warehouse (inventory) and Customer Access List. - Attach APIs to link categories and product management to the warehouse. - After linking, the ADMIN section is done. 10/11/2025 16/11/2025 http://localhost:3001/ 5 - Continue attaching APIs from the backend and test the functionalities. 10/11/2025 16/11/2025 http://localhost:3001/ 6 - Practice the workshop and write the worklog and prepare the worklog and workshop. + Create an EC2 instance + Connect via SSH + Attach an EBS volume 10/11/2025 16/11/2025 http://localhost:3001/ Week 10 Results: Completed API integration into frontend:\nAttached APIs module by module, tested using DevTools (F12). Eliminated/significantly reduced minor bugs. Organization \u0026amp; work coordination:\nAgreed on offline meeting schedule. Clarified project roadmap for upcoming milestones. Backend support \u0026amp; API completion:\nCollaborated with BE to add necessary features. Continued completing remaining APIs. Data expansion \u0026amp; administration:\nAdded warehouse (inventory) and customer access list. Linked categories \u0026amp; product management to warehouse. ADMIN section completion \u0026amp; testing:\nCompleted ADMIN module after data linking. Continued end-to-end functionality testing. Infrastructure workshop:\nCreated EC2 instance, SSH connection, attached EBS volume. Completed and updated worklog. \u0026hellip;\n"},{"uri":"https://thienluhoan.github.io/workshop-template/1-worklog/1.11-week11/","title":"Week 11 Worklog","tags":[],"description":"","content":"Week 11 Objectives: Complete the basic project to prepare for submission. Complete workshop. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Code remaining features.\n- Demo and prepare for the meeting. 17/11/2025 23/11/2025 https://github.com/khanhtm45/AWS 3 - Meeting and prepare documentation for backend and frontend. - Allocate tasks reasonably, accelerate progress to meet project deadline and submit on time. 17/11/2025 23/11/2025 https://github.com/khanhtm45/AWS 4 - Complete workshop with the whole team. - Study and learn. 17/11/2025 23/11/2025 https://github.com/vanhoangkha/AWS-First-Cloud-Journey?tab=readme-ov-file 5 - Study and research applications, implement into the project if time permits. 17/11/2025 23/11/2025 https://us-east-1.console.aws.amazon.com/ec2/home?region=us-east-1 6 - Work with the team to minimize bugs as much as possible.\n- Check login \u0026amp; logout to ensure no errors occur during submission week demo. 17/11/2025 23/11/2025 https://github.com/khanhtm45/AWS Week 11 Results: Product Completion: Completed coding of the project\u0026rsquo;s missing functions and integrated components.\nQuality Assurance (QA): Reviewed and fixed minor bugs (bug fixing) with the team. Ensured important features such as Login \u0026amp; Logout worked stably, no errors occurred during the demo.\nProject Documentation: Completed drafting detailed technical documents (docx) for both Backend and Frontend.\nProgress Management: Had team meetings, divided work reasonably and promoted progress to ensure timely submission.\nLearning Activities: Completed the Workshop with the team and learned more about extended applications/services on AWS to optimize the project.\n"},{"uri":"https://thienluhoan.github.io/workshop-template/1-worklog/1.12-week12/","title":"Week 12 Worklog","tags":[],"description":"","content":" ‚ö†Ô∏è Note: The following information is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nWeek 12 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Stop adding new features, only focus on final system review. - Check AWS connections (DB, EC2, S3\u0026hellip;) for any intermittent errors. 24/11/2025 01/12/2025 https://github.com/khanhtm45/AWS 3 - Compile documentation (Report, Slides, Source code). 24/11/2025 01/12/2025 https://github.com/khanhtm45/AWS 4 - Prepare presentation slides: Summarize architecture, technologies used and project highlights. - Divide speaking parts for each team member. 24/11/2025 01/12/2025 https://github.com/khanhtm45/AWS 5 - Run demo rehearsal: Simulate presentation to align timing and operations. - Prepare answers for potential Q\u0026amp;A questions. 24/11/2025 01/12/2025 https://github.com/khanhtm45/AWS 6 - Review AWS theoretical knowledge related to the project for exam/Q\u0026amp;A preparation. - Keep AWS server in the most stable state. 24/11/2025 01/12/2025 https://github.com/khanhtm45/AWS Week 12 Results: Completion: Full Source code, Report and Technical documentation submitted on time as required.\nDemo ready:\nAWS system thoroughly tested, running smoothly according to demo scenario. Sample data prepared and ready to demonstrate features. Presentation preparation:\nCompleted presentation slides (Presentation). Agreed on speaking script and clearly divided demo roles among members. Mentally prepared and knowledgeable for project defense. "},{"uri":"https://thienluhoan.github.io/workshop-template/3-blogstranslated/","title":"Translated Blogs","tags":[],"description":"","content":"This section will list and introduce the blogs you have translated. For example:\nBlog 1 ‚Äì Guide to 6 strategies and tools for cloud cost optimization on AWS, Azure, and GCP. The article focuses on providing 6 core strategies and practical tools to optimize cloud costs systematically across AWS, Azure, and GCP. The key point is that cost optimization is a continuous and disciplined effort, starting with Right-size resources (adjust resource sizes) and clean up unused resources to avoid waste. Other important strategies include storage cost optimization by choosing the right storage tier, reducing network egress fees by managing data flows, and using each platform‚Äôs built-in optimization recommendations (such as Recommender, Trusted Advisor). The ultimate goal is to set up continuous monitoring (budget, alert) to maintain spending performance, helping organizations sustainably reduce costs, increase flexibility, and maintain confidence in their cloud strategy. With a disciplined FinOps practice, you will sustainably reduce costs, increase flexibility, and maintain leadership‚Äôs confidence in the cloud strategy.\nBlog 2 ‚Äì Seamless streaming with preconditioned ads from Olyzon and AWS Elemental MediaTailor The article focuses on a technology solution combining AWS Elemental MediaTailor and Olyzon to solve streaming interruptions caused by ads. The core idea is using preconditioned ads, allowing ads to be pre-transcoded and stitched into the content stream seamlessly, eliminating latency and buffering. Olyzon adds L-shape overlay technology to create less intrusive ads. This solution helps service providers maximize ad revenue (thanks to reliability and no missed impressions) while ensuring an optimal viewer experience.\nBlog 3 ‚Äì Register now: Updated exam and new name for the cloud operations certification The article focuses on the update and renaming of the AWS Certified SysOps Administrator ‚Äì Associate certification to AWS Certified CloudOps Engineer ‚Äì Associate (New code: SOA-C03), aiming to reflect the evolution of the Cloud Operations role and changing industry terminology. The key point is the change in the exam‚Äôs knowledge scope, including the addition of Containers, a stronger emphasis on automation, Infrastructure as Code, and operating in multi-account/multi-Region environments. This change enables professionals to demonstrate the most modern skills in deploying, operating, and maintaining workloads on AWS, thereby maximizing the credibility and relevance of the certification in the industry.\nBlog 4 - \u0026hellip; This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices‚Ä¶), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\nBlog 5 - \u0026hellip; This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices‚Ä¶), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\nBlog 6 - \u0026hellip; This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices‚Ä¶), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\n"},{"uri":"https://thienluhoan.github.io/workshop-template/4-eventparticipated/","title":"Events Participated","tags":[],"description":"","content":"Event 1 Event Name: GenAI Open Innovation Vietnam 2025\nDate \u0026amp; Time: 09:00, 18 September 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nEvent 2 Event Name: AI-powered planning, design, and coding for modern software development\nDate \u0026amp; Time: 10:00, 01 October 2025\nLocation: At home via GoToWebinar\u0026rsquo;s meeting/recording room.\nRole: Attendee\n"},{"uri":"https://thienluhoan.github.io/workshop-template/5-workshop/5.4-setup_lambda/","title":"Setup Lambda Function","tags":[],"description":"","content":"1. Navigate to AWS Lambda\nOpen the AWS Lambda Console. Lambda supports multiple programming languages. For this workshop, we will use Node.js.\n2. Click Create Function\nEnter function name: BedrockChatbotHandler Select runtime: Node.js 24.x Click Change default execution role Select Use an existing role Choose the IAM role you created in Prerequisites (with AmazonBedrockFullAccess) Click Create function 3. Configure Function Settings\nNavigate to the Configuration tab:\nSet Memory to 512 MB Set Timeout to 2 minutes 4. Deploy Function Code\nGo to the Code tab and paste the following code:\nconst { BedrockRuntimeClient, InvokeModelCommand } = require(\u0026#34;@aws-sdk/client-bedrock-runtime\u0026#34;); const client = new BedrockRuntimeClient({ region: \u0026#34;us-east-1\u0026#34; }); exports.handler = async (event) =\u0026gt; { try { // Parse request body const body = JSON.parse(event.body || \u0026#39;{}\u0026#39;); const userMessage = body.message; const conversationHistory = body.history || []; // Validate input if (!userMessage || userMessage.trim().length === 0) { return { statusCode: 400, headers: { \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39;, \u0026#39;Access-Control-Allow-Origin\u0026#39;: \u0026#39;*\u0026#39; }, body: JSON.stringify({ error: \u0026#39;Message is required\u0026#39; }) }; } // Build conversation for Claude const messages = [ ...conversationHistory, { role: \u0026#34;user\u0026#34;, content: userMessage } ]; // Prepare Bedrock request const input = { modelId: \u0026#34;us.anthropic.claude-3-5-haiku-20241022-v1:0\u0026#34;, contentType: \u0026#34;application/json\u0026#34;, accept: \u0026#34;application/json\u0026#34;, body: JSON.stringify({ anthropic_version: \u0026#34;bedrock-2023-05-31\u0026#34;, max_tokens: 1024, messages: messages }) }; // Invoke Bedrock const command = new InvokeModelCommand(input); const response = await client.send(command); // Parse response const responseBody = JSON.parse(new TextDecoder().decode(response.body)); const assistantMessage = responseBody.content[0].text; return { statusCode: 200, headers: { \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39;, \u0026#39;Access-Control-Allow-Origin\u0026#39;: \u0026#39;*\u0026#39; }, body: JSON.stringify({ message: assistantMessage, conversationHistory: [ ...messages, { role: \u0026#34;assistant\u0026#34;, content: assistantMessage } ] }) }; } catch (error) { console.error(\u0026#39;Error:\u0026#39;, error); return { statusCode: 500, headers: { \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39;, \u0026#39;Access-Control-Allow-Origin\u0026#39;: \u0026#39;*\u0026#39; }, body: JSON.stringify({ error: \u0026#39;Internal server error\u0026#39;, details: error.message }) }; } }; 5. Click Deploy\nAfter pasting the code, click the Deploy button to save your Lambda function.\nKey Points:\nThe function validates user input Builds conversation history for context Calls Bedrock\u0026rsquo;s Claude model Returns AI response with updated conversation history Includes CORS headers for browser access "},{"uri":"https://thienluhoan.github.io/workshop-template/5-workshop/5.5-setup-api-gateway/","title":"Setup API Gateway","tags":[],"description":"","content":"In this section, you will create a REST API using Amazon API Gateway that will serve as the entry point for your chatbot. The API will receive requests from the frontend and invoke your Lambda function.\n1. Navigate to API Gateway Console\nOpen the API Gateway Console\n2. Create REST API\nClick Create API Choose REST API (not Private or HTTP API) Click Build Select New API Enter API name: BedrockChatbotAPI Click Create API 3. Create Resource\nClick Actions ‚Üí Create Resource Resource Name: chat Click Create Resource 4. Create POST Method\nSelect the /chat resource Click Actions ‚Üí Create Method Select POST from dropdown Click the checkmark Integration type: Lambda Function Enable Lambda Proxy Integration Select your Lambda function: BedrockChatbotHandler Click Save Click OK to grant API Gateway permission 5. Enable CORS\nSelect the /chat resource Click Actions ‚Üí Enable CORS Keep default settings Click Enable CORS and replace existing CORS headers Click Yes, replace existing values 6. Deploy API\nClick Actions ‚Üí Deploy API Deployment stage: [New Stage] Stage name: prod Click Deploy 7. Save Your API Endpoint\nAfter deployment, you will see an Invoke URL at the top of the page. It will look like:\nhttps://xxxxxxxxxx.execute-api.us-east-1.amazonaws.com/prod Copy this URL - you will need it for your frontend configuration. Your complete endpoint will be:\nhttps://xxxxxxxxxx.execute-api.us-east-1.amazonaws.com/prod/chat 8. Test Your API (Optional)\nYou can test using curl or Postman:\ncurl -X POST https://your-api-id.execute-api.us-east-1.amazonaws.com/prod/chat \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{\u0026#34;message\u0026#34;: \u0026#34;Hello, who are you?\u0026#34;}\u0026#39; Expected response:\n{ \u0026#34;message\u0026#34;: \u0026#34;Hello! I\u0026#39;m Claude, an AI assistant created by Anthropic...\u0026#34;, \u0026#34;conversationHistory\u0026#34;: [...] } Key Points:\nREST API provides HTTPS endpoint for frontend CORS enabled for browser access Lambda proxy integration passes entire request to Lambda API Gateway handles authentication and throttling Connect to an EC2 instance and verify connectivity to S3 Start a new AWS Session Manager session on the instance named Test-Gateway-Endpoint. From the session, verify that you can list the contents of the bucket you created in Part 1: Access S3 from VPC: aws s3 ls s3://\\\u0026lt;your-bucket-name\\\u0026gt; The bucket contents include the two 1 GB files uploaded in earlier.\nCreate a new S3 bucket; follow the naming pattern you used in Part 1, but add a \u0026lsquo;-2\u0026rsquo; to the name. Leave other fields as default and click create Successfully create bucket\nNavigate to: Services \u0026gt; VPC \u0026gt; Endpoints, then select the Gateway VPC endpoint you created earlier. Click the Policy tab. Click Edit policy. The default policy allows access to all S3 Buckets through the VPC endpoint.\nIn Edit Policy console, copy \u0026amp; Paste the following policy, then replace yourbucketname-2 with your 2nd bucket name. This policy will allow access through the VPC endpoint to your new bucket, but not any other bucket in Amazon S3. Click Save to apply the policy. {\r\u0026#34;Id\u0026#34;: \u0026#34;Policy1631305502445\u0026#34;,\r\u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;,\r\u0026#34;Statement\u0026#34;: [\r{\r\u0026#34;Sid\u0026#34;: \u0026#34;Stmt1631305501021\u0026#34;,\r\u0026#34;Action\u0026#34;: \u0026#34;s3:*\u0026#34;,\r\u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;,\r\u0026#34;Resource\u0026#34;: [\r\u0026#34;arn:aws:s3:::yourbucketname-2\u0026#34;,\r\u0026#34;arn:aws:s3:::yourbucketname-2/*\u0026#34;\r],\r\u0026#34;Principal\u0026#34;: \u0026#34;*\u0026#34;\r}\r]\r} Successfully customize policy\nFrom your session on the Test-Gateway-Endpoint instance, test access to the S3 bucket you created in Part 1: Access S3 from VPC aws s3 ls s3://\u0026lt;yourbucketname\u0026gt; This command will return an error because access to this bucket is not permitted by your new VPC endpoint policy:\nReturn to your home directory on your EC2 instance cd~ Create a file fallocate -l 1G test-bucket2.xyz Copy file to 2nd bucket aws s3 cp test-bucket2.xyz s3://\u0026lt;your-2nd-bucket-name\u0026gt; This operation succeeds because it is permitted by the VPC endpoint policy.\nThen we test access to the first bucket by copy the file to 1st bucket aws s3 cp test-bucket2.xyz s3://\u0026lt;your-1st-bucket-name\u0026gt; This command will return an error because access to this bucket is not permitted by your new VPC endpoint policy.\nPart 3 Summary: In this section, you created a VPC endpoint policy for Amazon S3, and used the AWS CLI to test the policy. AWS CLI actions targeted to your original S3 bucket failed because you applied a policy that only allowed access to the second bucket you created. AWS CLI actions targeted for your second bucket succeeded because the policy allowed them. These policies can be useful in situations where you need to control access to resources through VPC endpoints.\n"},{"uri":"https://thienluhoan.github.io/workshop-template/5-workshop/","title":"Workshop","tags":[],"description":"","content":" ‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nBuilding a Serverless AI Chatbot with Amazon Bedrock Overview Amazon Bedrock provides access to leading foundation models (LLMs) from Anthropic, Meta, AI21 Labs and other providers through a simple API, allowing you to build AI applications without managing complex infrastructure.\nIn this workshop, you will learn how to build, deploy, and test a complete AI Chatbot using serverless architecture that allows users to interact with Claude Haiku 4.5 without managing servers or worrying about scaling.\nWe will create a system with 5 main components to build the chatbot: Amazon Bedrock (AI engine), AWS Lambda (backend logic), API Gateway (REST API endpoint), Amazon S3 (frontend hosting), and CloudWatch (monitoring). These components provide a complete serverless architecture with low cost and automatic scaling capabilities.\nMain Components: Amazon Bedrock (AI Engine) - Provides Claude Haiku 4.5 model through simple API AWS Lambda (Backend Logic) - Runs Node.js code to process requests from API Gateway API Gateway (REST API Endpoint) - Creates public HTTPS endpoint for frontend calls Amazon S3 (Frontend Hosting) - Hosts static website for chatbot UI CloudWatch (Monitoring \u0026amp; Debugging) - Automatically collects logs from Lambda execution Content Workshop Overview Prerequisites Setup Amazon Bedrock Setup Lambda Function Setup API Gateway Monitoring with CloudWatch Setup S3 Static Website Clean up "},{"uri":"https://thienluhoan.github.io/workshop-template/5-workshop/5.6-monitoring-cloudwatch/","title":"Monitoring with CloudWatch","tags":[],"description":"","content":"AWS Lambda automatically integrates with Amazon CloudWatch to provide monitoring and logging for your serverless applications. In this section, you\u0026rsquo;ll learn how to view and analyze your chatbot\u0026rsquo;s logs.\n1. Navigate to Lambda Function\nOpen your Lambda function BedrockChatbotHandler in the AWS Console.\n2. View Logs in Monitor Tab\nClick on the Monitor tab Click View CloudWatch logs This will take you to CloudWatch Logs where all your Lambda execution logs are stored.\n3. Explore Log Streams\nEach Lambda invocation creates log entries in log streams Click on a log stream to view detailed execution logs You can see: Request/response data Execution duration Memory usage Any errors or exceptions 4. Useful CloudWatch Features\nSearch Logs:\nUse the search bar to filter logs Search for specific error messages or patterns Set Up Alerts:\nCreate CloudWatch Alarms for error rates Get notified when errors exceed threshold View Metrics:\nInvocation count Error rate Duration Throttles 5. Common Debugging Scenarios\nIf chatbot doesn\u0026rsquo;t respond:\nCheck CloudWatch logs for error messages Verify IAM role has Bedrock permissions Check if model ID is correct If API Gateway returns 502:\nLambda function timeout (increase timeout) Lambda function error (check logs) If CORS errors in browser:\nVerify CORS is enabled in API Gateway Check Lambda response includes CORS headers 6. Best Practices\nRegularly review CloudWatch logs Set up alarms for error rates Monitor Lambda costs and execution time Use structured logging for better analysis CloudWatch is essential for maintaining and troubleshooting your serverless chatbot!\n"},{"uri":"https://thienluhoan.github.io/workshop-template/6-self-evaluation/","title":"Self-Assessment","tags":[],"description":"","content":" ‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy it verbatim into your report, including this warning.\nDuring my internship at [Company/Organization Name] from [start date] to [end date], I had the opportunity to learn, practice, and apply the knowledge acquired in school to a real-world working environment.\nI participated in [briefly describe the main project or task], through which I improved my skills in [list skills: programming, analysis, reporting, communication, etc.].\nIn terms of work ethic, I always strived to complete tasks well, complied with workplace regulations, and actively engaged with colleagues to improve work efficiency.\nTo objectively reflect on my internship period, I would like to evaluate myself based on the following criteria:\nNo. Criteria Description Good Fair Average 1 Professional knowledge \u0026amp; skills Understanding of the field, applying knowledge in practice, proficiency with tools, work quality ‚úÖ ‚òê ‚òê 2 Ability to learn Ability to absorb new knowledge and learn quickly ‚òê ‚úÖ ‚òê 3 Proactiveness Taking initiative, seeking out tasks without waiting for instructions ‚úÖ ‚òê ‚òê 4 Sense of responsibility Completing tasks on time and ensuring quality ‚úÖ ‚òê ‚òê 5 Discipline Adhering to schedules, rules, and work processes ‚òê ‚òê ‚úÖ 6 Progressive mindset Willingness to receive feedback and improve oneself ‚òê ‚úÖ ‚òê 7 Communication Presenting ideas and reporting work clearly ‚òê ‚úÖ ‚òê 8 Teamwork Working effectively with colleagues and participating in teams ‚úÖ ‚òê ‚òê 9 Professional conduct Respecting colleagues, partners, and the work environment ‚úÖ ‚òê ‚òê 10 Problem-solving skills Identifying problems, proposing solutions, and showing creativity ‚òê ‚úÖ ‚òê 11 Contribution to project/team Work effectiveness, innovative ideas, recognition from the team ‚úÖ ‚òê ‚òê 12 Overall General evaluation of the entire internship period ‚úÖ ‚òê ‚òê Needs Improvement Strengthen discipline and strictly comply with the rules and regulations of the company or any organization Improve problem-solving thinking Enhance communication skills in both daily interactions and professional contexts, including handling situations effectively "},{"uri":"https://thienluhoan.github.io/workshop-template/5-workshop/5.7-setup-s3/","title":"Setup S3 Static Website","tags":[],"description":"","content":"In this final section, you will deploy your chatbot frontend to Amazon S3 as a static website. This provides a cost-effective way to host your web application.\n1. Create S3 Bucket\nOpen the S3 Console Click Create bucket Enter bucket name: my-bedrock-chatbot-frontend (must be globally unique) Select your region Uncheck \u0026ldquo;Block all public access\u0026rdquo; Acknowledge the warning about public access Click Create bucket 2. Enable Static Website Hosting\nClick on your bucket name Go to Properties tab Scroll down to Static website hosting Click Edit Select Enable Index document: index.html Click Save changes Note the Bucket website endpoint URL 3. Add Bucket Policy for Public Access\nGo to Permissions tab Scroll to Bucket policy Click Edit Paste the following policy (replace YOUR-BUCKET-NAME): { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;PublicReadGetObject\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: \u0026#34;*\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;s3:GetObject\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:s3:::YOUR-BUCKET-NAME/*\u0026#34; } ] } Click Save changes 4. Create Frontend HTML File\nCreate a file named index.html with the following content:\n\u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;UTF-8\u0026#34;\u0026gt; \u0026lt;meta name=\u0026#34;viewport\u0026#34; content=\u0026#34;width=device-width, initial-scale=1.0\u0026#34;\u0026gt; \u0026lt;title\u0026gt;AI Chatbot with Claude\u0026lt;/title\u0026gt; \u0026lt;style\u0026gt; * { margin: 0; padding: 0; box-sizing: border-box; } body { font-family: \u0026#39;Segoe UI\u0026#39;, Tahoma, Geneva, Verdana, sans-serif; background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); height: 100vh; display: flex; justify-content: center; align-items: center; } .container { width: 90%; max-width: 800px; height: 90vh; background: white; border-radius: 20px; box-shadow: 0 20px 60px rgba(0,0,0,0.3); display: flex; flex-direction: column; } .header { background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: white; padding: 20px; border-radius: 20px 20px 0 0; text-align: center; } .chat-container { flex: 1; overflow-y: auto; padding: 20px; background: #f5f5f5; } .message { margin-bottom: 15px; display: flex; align-items: flex-start; } .message.user { justify-content: flex-end; } .message-content { max-width: 70%; padding: 12px 18px; border-radius: 18px; word-wrap: break-word; } .message.user .message-content { background: #667eea; color: white; } .message.assistant .message-content { background: white; color: #333; border: 1px solid #e0e0e0; } .input-container { padding: 20px; background: white; border-radius: 0 0 20px 20px; display: flex; gap: 10px; } #userInput { flex: 1; padding: 12px 18px; border: 2px solid #e0e0e0; border-radius: 25px; font-size: 16px; outline: none; } #userInput:focus { border-color: #667eea; } #sendButton { padding: 12px 30px; background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: white; border: none; border-radius: 25px; cursor: pointer; font-size: 16px; font-weight: bold; } #sendButton:hover { transform: translateY(-2px); } #sendButton:disabled { background: #ccc; cursor: not-allowed; } .loading { display: none; text-align: center; padding: 10px; color: #667eea; } \u0026lt;/style\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;div class=\u0026#34;container\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;header\u0026#34;\u0026gt; \u0026lt;h1\u0026gt;ü§ñ AI Chatbot with Claude\u0026lt;/h1\u0026gt; \u0026lt;p\u0026gt;Powered by Amazon Bedrock\u0026lt;/p\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div class=\u0026#34;chat-container\u0026#34; id=\u0026#34;chatContainer\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;message assistant\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;message-content\u0026#34;\u0026gt; Hello! I\u0026#39;m Claude, your AI assistant. How can I help you today? \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div class=\u0026#34;loading\u0026#34; id=\u0026#34;loading\u0026#34;\u0026gt;Claude is thinking...\u0026lt;/div\u0026gt; \u0026lt;div class=\u0026#34;input-container\u0026#34;\u0026gt; \u0026lt;input type=\u0026#34;text\u0026#34; id=\u0026#34;userInput\u0026#34; placeholder=\u0026#34;Type your message...\u0026#34; onkeypress=\u0026#34;if(event.key===\u0026#39;Enter\u0026#39;) sendMessage()\u0026#34;\u0026gt; \u0026lt;button id=\u0026#34;sendButton\u0026#34; onclick=\u0026#34;sendMessage()\u0026#34;\u0026gt;Send\u0026lt;/button\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;script\u0026gt; // IMPORTANT: Replace with your API Gateway endpoint const API_ENDPOINT = \u0026#39;https://YOUR-API-ID.execute-api.us-east-1.amazonaws.com/prod/chat\u0026#39;; let conversationHistory = []; function addMessage(role, content) { const chatContainer = document.getElementById(\u0026#39;chatContainer\u0026#39;); const messageDiv = document.createElement(\u0026#39;div\u0026#39;); messageDiv.className = `message ${role}`; messageDiv.innerHTML = `\u0026lt;div class=\u0026#34;message-content\u0026#34;\u0026gt;${content}\u0026lt;/div\u0026gt;`; chatContainer.appendChild(messageDiv); chatContainer.scrollTop = chatContainer.scrollHeight; } async function sendMessage() { const userInput = document.getElementById(\u0026#39;userInput\u0026#39;); const sendButton = document.getElementById(\u0026#39;sendButton\u0026#39;); const loading = document.getElementById(\u0026#39;loading\u0026#39;); const message = userInput.value.trim(); if (!message) return; // Disable input userInput.disabled = true; sendButton.disabled = true; loading.style.display = \u0026#39;block\u0026#39;; // Display user message addMessage(\u0026#39;user\u0026#39;, message); userInput.value = \u0026#39;\u0026#39;; try { const response = await fetch(API_ENDPOINT, { method: \u0026#39;POST\u0026#39;, headers: { \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39; }, body: JSON.stringify({ message: message, history: conversationHistory }) }); const data = await response.json(); if (response.ok) { addMessage(\u0026#39;assistant\u0026#39;, data.message); conversationHistory = data.conversationHistory; } else { addMessage(\u0026#39;assistant\u0026#39;, `Error: ${data.error || \u0026#39;Something went wrong\u0026#39;}`); } } catch (error) { addMessage(\u0026#39;assistant\u0026#39;, `Error: ${error.message}`); } finally { userInput.disabled = false; sendButton.disabled = false; loading.style.display = \u0026#39;none\u0026#39;; userInput.focus(); } } \u0026lt;/script\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; 5. Update API Endpoint\nIn the index.html file, replace:\nconst API_ENDPOINT = \u0026#39;https://YOUR-API-ID.execute-api.us-east-1.amazonaws.com/prod/chat\u0026#39;; With your actual API Gateway endpoint from section 5.5.\n6. Upload to S3\nGo back to your S3 bucket Click Upload Drag and drop your index.html file Click Upload 7. Access Your Chatbot\nGo to Properties tab Find Static website hosting Click on the Bucket website endpoint URL Your chatbot is now live! üéâ 8. Test Your Chatbot\nTry asking questions like:\n\u0026ldquo;What is Amazon Bedrock?\u0026rdquo; \u0026ldquo;Explain AWS Lambda in simple terms\u0026rdquo; \u0026ldquo;Tell me a joke\u0026rdquo; Congratulations! You have successfully built and deployed a serverless AI chatbot using Amazon Bedrock, Lambda, API Gateway, and S3!\nNext Steps:\nAdd authentication with Amazon Cognito Implement conversation memory with DynamoDB Add file upload capabilities Deploy with custom domain using Route 53 "},{"uri":"https://thienluhoan.github.io/workshop-template/7-feedback/","title":"Sharing and Feedback","tags":[],"description":"","content":" ‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nHere, you can freely share your personal opinions about your experience participating in the First Cloud Journey program. This will help the FCJ team improve any shortcomings based on the following aspects:\nOverall Evaluation 1. Working Environment\nThe working environment is very friendly and open. FCJ members are always willing to help whenever I encounter difficulties, even outside working hours. The workspace is tidy and comfortable, helping me focus better. However, I think it would be nice to have more social gatherings or team bonding activities to strengthen relationships.\n2. Support from Mentor / Team Admin\nThe mentor provides very detailed guidance, explains clearly when I don‚Äôt understand, and always encourages me to ask questions. The admin team supports administrative tasks, provides necessary documents, and creates favorable conditions for me to work effectively. I especially appreciate that the mentor allows me to try and solve problems myself instead of just giving the answer.\n3. Relevance of Work to Academic Major\nThe tasks I was assigned align well with the knowledge I learned at university, while also introducing me to new areas I had never encountered before. This allowed me to both strengthen my foundational knowledge and gain practical skills.\n4. Learning \u0026amp; Skill Development Opportunities\nDuring the internship, I learned many new skills such as using project management tools, teamwork skills, and professional communication in a corporate environment. The mentor also shared valuable real-world experiences that helped me better plan my career path.\n5. Company Culture \u0026amp; Team Spirit\nThe company culture is very positive: everyone respects each other, works seriously but still keeps things enjoyable. When there are urgent projects, everyone works together and supports one another regardless of their position. This made me feel like a real part of the team, even as an intern.\n6. Internship Policies / Benefits\nThe company provides an internship allowance and offers flexible working hours when needed. In addition, having the opportunity to join internal training sessions is a big plus.\nAdditional Questions What did you find most satisfying during your internship? What do you think the company should improve for future interns? If recommending to a friend, would you suggest they intern here? Why or why not? Suggestions \u0026amp; Expectations Do you have any suggestions to improve the internship experience? Would you like to continue this program in the future? Any other comments (free sharing): "},{"uri":"https://thienluhoan.github.io/workshop-template/categories/","title":"Categories","tags":[],"description":"","content":""},{"uri":"https://thienluhoan.github.io/workshop-template/tags/","title":"Tags","tags":[],"description":"","content":""}]